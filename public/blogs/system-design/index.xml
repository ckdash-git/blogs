<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>System Design on Chandan Kumar Dash</title>
        <link>http://localhost:55562/blogs/system-design/</link>
        <description>Recent content in System Design on Chandan Kumar Dash</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Sun, 08 Feb 2026 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:55562/blogs/system-design/index.xml" rel="self" type="application/rss+xml" /><item>
            <title>Availability &amp; Fault Tolerance: Keeping the Lights On</title>
            <link>http://localhost:55562/blogs/system-design/availability-and-fault-tolerance/</link>
            <pubDate>Sun, 08 Feb 2026 00:00:00 +0000</pubDate>
            <guid>http://localhost:55562/blogs/system-design/availability-and-fault-tolerance/</guid>
            <description>&lt;p&gt;Imagine it&amp;rsquo;s the day of your board exam results. You, along with millions of other students, log in to the result portal at 10:00 AM. The site crashes. That is an &lt;strong&gt;Availability&lt;/strong&gt; failure.&lt;/p&gt;&#xA;&lt;p&gt;In previous posts, we discussed &lt;a class=&#34;link&#34; href=&#34;http://localhost:55562/blogs/system-design/throughput-in-distributed-systems/&#34; &gt;Throughput&lt;/a&gt; and &lt;a class=&#34;link&#34; href=&#34;http://localhost:55562/blogs/system-design/latency-in-web-applications/&#34; &gt;Latency&lt;/a&gt;. Today, we tackle the most critical aspect of any production system: &lt;strong&gt;staying online&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;h2 id=&#34;core-concepts&#34;&gt;Core Concepts&#xA;&lt;/h2&gt;&lt;h3 id=&#34;1-availability&#34;&gt;1. Availability&#xA;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Availability&lt;/strong&gt; is the percentage of time a system is operational and accessible to users. It&amp;rsquo;s often measured in &amp;ldquo;nines&amp;rdquo;:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;99%&lt;/strong&gt;: Down for 3.65 days/year.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;99.99% (&amp;ldquo;Four Nines&amp;rdquo;)&lt;/strong&gt;: Down for 52 minutes/year.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;99.999% (&amp;ldquo;Five Nines&amp;rdquo;)&lt;/strong&gt;: Down for 5 minutes/year.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;2-fault-tolerance&#34;&gt;2. Fault Tolerance&#xA;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Fault Tolerance&lt;/strong&gt; is the ability of a system to continue operating properly in the event of the failure of some of its components.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;em&gt;Example&lt;/em&gt;: If one engine of an airplane fails, the plane can still fly. The plane is Fault Tolerant.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;3-single-point-of-failure-spof&#34;&gt;3. Single Point of Failure (SPOF)&#xA;&lt;/h3&gt;&lt;p&gt;A part of a system that, if it fails, stops the entire system from working.&lt;/p&gt;&#xA;&lt;h2 id=&#34;monoliths-vs-distributed-systems&#34;&gt;Monoliths vs. Distributed Systems&#xA;&lt;/h2&gt;&lt;p&gt;The architectural choice you make dictates your system&amp;rsquo;s survival strategy.&lt;/p&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Feature&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Monolithic Architecture&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Distributed System&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Structure&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;All-in-one bundle&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Modular, spread across nodes&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Failure Mode&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;SPOF&lt;/strong&gt;: If the server crashes, everything dies.&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Resilient&lt;/strong&gt;: If one node dies, others take over.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Availability&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Low (Requires downtime for updates/crashes)&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;High (Zero-downtime deployments)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Recovery&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Reboot the entire beast&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Automatic failover&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;pre class=&#34;mermaid&#34;&gt;&#xA;  graph TD&#xA;    subgraph Monolith [Monolithic System - SPOF]&#xA;        User1[User] --&amp;gt; Server[Single Server]&#xA;        Server -.-&amp;gt; Crash[X Crash]&#xA;        Crash -.-&amp;gt; Down[System Offline]&#xA;        style Server fill:#ff9999&#xA;    end&#xA;&#xA;    subgraph Distributed [Distributed System - Fault Tolerant]&#xA;        User2[User] --&amp;gt; LB[Load Balancer]&#xA;        LB --&amp;gt; NodeA[Node A]&#xA;        LB --&amp;gt; NodeB[Node B]&#xA;        NodeA -.-&amp;gt; Fail[X Fail]&#xA;        LB ==Failover==&amp;gt; NodeB&#xA;        style NodeA fill:#ff9999&#xA;        style NodeB fill:#99ff99&#xA;    end&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;h2 id=&#34;the-secret-sauce-replication&#34;&gt;The Secret Sauce: Replication&#xA;&lt;/h2&gt;&lt;p&gt;How do distributed systems achieve high availability? &lt;strong&gt;Redundancy&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;p&gt;We don&amp;rsquo;t just rely on one server. We &lt;strong&gt;Replicate&lt;/strong&gt; everything.&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;Application Replication&lt;/strong&gt;: Run the same code on 10 different servers. If 3 crash, 7 are still running.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Data Replication&lt;/strong&gt;: Store your user data on a primary database and sync it to a standby replica. If the primary burns down, the standby takes over.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Geographic Replication&lt;/strong&gt;: Don&amp;rsquo;t put all servers in one data center. If the entire underlying power grid of a region fails, your app typically keeps running from a different region.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;the-acid-trade-off&#34;&gt;The ACID Trade-off&#xA;&lt;/h3&gt;&lt;p&gt;In databases, replication introduces complexity. If you write data to Node A, it takes time to copy to Node B. This touches on the &lt;strong&gt;CAP Theorem&lt;/strong&gt; (Consistency vs. Availability), which we will cover in depth later.&lt;/p&gt;&#xA;&lt;h2 id=&#34;real-life-examples&#34;&gt;Real-Life Examples&#xA;&lt;/h2&gt;&lt;h3 id=&#34;1-amazon-shopping-prime-day&#34;&gt;1. Amazon Shopping (Prime Day)&#xA;&lt;/h3&gt;&lt;p&gt;On Prime Day, traffic spikes 100x. Amazon uses thousands of microservices distributed across the globe. If the &amp;ldquo;Reviews&amp;rdquo; service crashes, you can still buy items. The system degrades gracefully rather than failing completely.&lt;/p&gt;&#xA;&lt;h3 id=&#34;2-google-search&#34;&gt;2. Google Search&#xA;&lt;/h3&gt;&lt;p&gt;Google indexes the web across thousands of machines. If the specific server holding the index for &amp;ldquo;SpaceX&amp;rdquo; fails, a replica immediately answers your query. You, the user, never know a failure occurred.&lt;/p&gt;&#xA;&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&#xA;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Monoliths put all eggs in one basket.&lt;/strong&gt; If that basket drops, you have a mess.&#xA;&lt;strong&gt;Distributed Systems&lt;/strong&gt; accept that failure is inevitable. Hard drives die, networks cut out, and power fails. By designing with &lt;strong&gt;Fault Tolerance&lt;/strong&gt; and &lt;strong&gt;Replication&lt;/strong&gt; in mind, we build systems that can survive the chaos of the real world.&lt;/p&gt;&#xA;</description>
        </item><item>
            <title>Consistency in Distributed Systems: The Data Truth</title>
            <link>http://localhost:55562/blogs/system-design/consistency-in-distributed-systems/</link>
            <pubDate>Sun, 08 Feb 2026 00:00:00 +0000</pubDate>
            <guid>http://localhost:55562/blogs/system-design/consistency-in-distributed-systems/</guid>
            <description>&lt;p&gt;In our last post on &lt;a class=&#34;link&#34; href=&#34;http://localhost:55562/blogs/system-design/availability-and-fault-tolerance/&#34; &gt;Availability&lt;/a&gt;, we saw that &lt;strong&gt;Replication&lt;/strong&gt; is key to keeping systems alive. But Replication introduces a new problem: &lt;strong&gt;Consistency&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;p&gt;If you have two copies of a database, and you update one, the other is instantly &amp;ldquo;stale.&amp;rdquo; Ideally, both would update instantly, but in the real world, that takes time.&lt;/p&gt;&#xA;&lt;h2 id=&#34;what-is-consistency&#34;&gt;What is Consistency?&#xA;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Consistency&lt;/strong&gt; means that every read receives the most recent write or an error. In simple terms: &lt;strong&gt;All clients see the same data at the same time.&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;real-life-example-the-atm-problem&#34;&gt;Real-Life Example: The ATM Problem&#xA;&lt;/h3&gt;&lt;p&gt;Imagine you have $500 in your bank account.&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;You&lt;/strong&gt; withdraw $500 from an ATM in Delhi.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Your Spouse&lt;/strong&gt; simultaneously tries to withdraw $500 from an ATM in Pune.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;In a &lt;strong&gt;Consistent&lt;/strong&gt; system, the second transaction fails because the balance is $0.&#xA;In an &lt;strong&gt;Inconsistent&lt;/strong&gt; system, both transactions might succeed, and the bank loses money.&lt;/p&gt;&#xA;&lt;h2 id=&#34;monoliths-vs-distributed-systems&#34;&gt;Monoliths vs. Distributed Systems&#xA;&lt;/h2&gt;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;System Type&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Consistency Level&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Why?&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Monolithic&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Naturally High&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Single database (Source of Truth). No sync needed.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Distributed&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Hard to Maintain&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Data is spread across nodes. Sync takes time (Network Latency).&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h2 id=&#34;types-of-consistency-models&#34;&gt;Types of Consistency Models&#xA;&lt;/h2&gt;&lt;p&gt;In distributed systems, you often have to choose between speed and accuracy.&lt;/p&gt;&#xA;&lt;h3 id=&#34;1-strong-consistency&#34;&gt;1. Strong Consistency&#xA;&lt;/h3&gt;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Definition&lt;/strong&gt;: Once a write is successful, &lt;strong&gt;all&lt;/strong&gt; subsequent reads return the new value.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Mechanism&lt;/strong&gt;: The system locks data during updates. Read operations wait until all replicas acknowledge the update.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Trade-off&lt;/strong&gt;: Higher Latency (slower).&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Use Case&lt;/strong&gt;: &lt;strong&gt;Banking Systems&lt;/strong&gt;, &lt;strong&gt;Stock Trading&lt;/strong&gt;, &lt;strong&gt;Train Ticket Booking&lt;/strong&gt; (IRCTC). You cannot sell the same seat twice.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre class=&#34;mermaid&#34;&gt;&#xA;  sequenceDiagram&#xA;    participant Client&#xA;    participant Primary&#xA;    participant Replica&#xA;    Client-&amp;gt;&amp;gt;Primary: Write(X=10)&#xA;    Primary-&amp;gt;&amp;gt;Replica: Sync(X=10)&#xA;    Replica--&amp;gt;&amp;gt;Primary: Acknowledge&#xA;    Primary--&amp;gt;&amp;gt;Client: Success&#xA;    Note right of Client: Now all reads see X=10&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;h3 id=&#34;2-eventual-consistency&#34;&gt;2. Eventual Consistency&#xA;&lt;/h3&gt;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Definition&lt;/strong&gt;: If no new updates are made, &lt;strong&gt;eventually&lt;/strong&gt; all accesses will return the last updated value.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Mechanism&lt;/strong&gt;: The system returns &amp;ldquo;Success&amp;rdquo; to the client immediately after updating the primary node. Replicas update in the background.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Trade-off&lt;/strong&gt;: Lower Latency (faster), but risk of stale data.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Use Case&lt;/strong&gt;: &lt;strong&gt;Social Media Feeds&lt;/strong&gt; (Instagram/Twitter). If you change your profile picture, it’s okay if your friend sees the old one for a few more seconds.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;3-weak-consistency&#34;&gt;3. Weak Consistency&#xA;&lt;/h3&gt;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Definition&lt;/strong&gt;: There is no guarantee that a read will return the most recent write. It relies on &amp;ldquo;best effort.&amp;rdquo;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Use Case&lt;/strong&gt;: &lt;strong&gt;Live Video Streaming&lt;/strong&gt;, &lt;strong&gt;VoIP&lt;/strong&gt;. If you miss a frame of video, it’s gone. The system doesn&amp;rsquo;t pause to sync.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;how-to-improve-consistency&#34;&gt;How to Improve Consistency&#xA;&lt;/h2&gt;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;Stop Read Operations&lt;/strong&gt;: During a major update, put the system in &amp;ldquo;Maintenance Mode.&amp;rdquo; (Brute force approach).&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Reduce Replica Distance&lt;/strong&gt;: Place replicas closer to each other to minimize sync time.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Application Coordination&lt;/strong&gt;: Use consensus algorithms like &lt;strong&gt;Paxos&lt;/strong&gt; or &lt;strong&gt;Raft&lt;/strong&gt; (used in Kubernetes/Etcd) to ensure nodes agree on the truth.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&#xA;&lt;/h2&gt;&lt;p&gt;Consistency is a spectrum.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Need &lt;strong&gt;Accuracy&lt;/strong&gt;? Choose &lt;strong&gt;Strong Consistency&lt;/strong&gt; (but accept slower performance).&lt;/li&gt;&#xA;&lt;li&gt;Need &lt;strong&gt;Speed&lt;/strong&gt;? Choose &lt;strong&gt;Eventual Consistency&lt;/strong&gt; (but accept temporary staleness).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;As a system architect, your job is to know which part of your app needs which model. Billing must be Strong; Recommendations can be Eventual.&lt;/p&gt;&#xA;</description>
        </item><item>
            <title>The CAP Theorem: You Can&#39;t Have It All</title>
            <link>http://localhost:55562/blogs/system-design/cap-theorem/</link>
            <pubDate>Sun, 08 Feb 2026 00:00:00 +0000</pubDate>
            <guid>http://localhost:55562/blogs/system-design/cap-theorem/</guid>
            <description>&lt;p&gt;In distributed systems, there is no free lunch. The &lt;strong&gt;CAP Theorem&lt;/strong&gt; (Brewer&amp;rsquo;s Theorem) states that a distributed data store can effectively provide only &lt;strong&gt;two&lt;/strong&gt; of the following three guarantees:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;Consistency (C)&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Availability (A)&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Partition Tolerance (P)&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;the-three-pillars&#34;&gt;The Three Pillars&#xA;&lt;/h2&gt;&lt;h3 id=&#34;1-consistency-c&#34;&gt;1. Consistency (C)&#xA;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Every read receives the most recent write or an error.&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;em&gt;Analogy&lt;/em&gt;: If you update your status on Facebook on your phone, your friend on their laptop should see the new status immediately. If they see the old one, the system is &lt;strong&gt;Inconsistent&lt;/strong&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;2-availability-a&#34;&gt;2. Availability (A)&#xA;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Every request receives a (non-error) response, without the guarantee that it contains the most recent write.&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;em&gt;Analogy&lt;/em&gt;: Even if the network is flaky, Amazon should allow you to add items to your cart. It’s better to sell the item and sync later than to show a &amp;ldquo;System Offline&amp;rdquo; error.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;3-partition-tolerance-p&#34;&gt;3. Partition Tolerance (P)&#xA;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;The system continues to operate despite an arbitrary number of messages being dropped or delayed by the network between nodes.&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;em&gt;Analogy&lt;/em&gt;: If the cable connecting the US and Europe servers is cut, the system should still work.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;the-reality-p-is-not-optional&#34;&gt;The Reality: P is Not Optional&#xA;&lt;/h2&gt;&lt;p&gt;In a Distributed System, network failures (Partitions) are inevitable. You cannot choose &amp;ldquo;CA&amp;rdquo; (Consistency + Availability) because that implies your network will never fail, which is impossible.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Therefore, the real choice is between CP and AP.&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;cp-consistency--partition-tolerance&#34;&gt;CP (Consistency + Partition Tolerance)&#xA;&lt;/h3&gt;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Philosophy&lt;/strong&gt;: &amp;ldquo;Better to return Error than Wrong Data.&amp;rdquo;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Behavior&lt;/strong&gt;: When a partition occurs, the system stops accepting writes to preserve consistency.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Use Case&lt;/strong&gt;: &lt;strong&gt;Banking&lt;/strong&gt;, &lt;strong&gt;ATM&lt;/strong&gt;. You cannot allow two peole to withdraw the same $100 if the ATMs lose connection. The ATM simply says &amp;ldquo;Out of Service&amp;rdquo; (Sacrificing Availability).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;ap-availability--partition-tolerance&#34;&gt;AP (Availability + Partition Tolerance)&#xA;&lt;/h3&gt;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Philosophy&lt;/strong&gt;: &amp;ldquo;The Show Must Go On.&amp;rdquo;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Behavior&lt;/strong&gt;: When a partition occurs, nodes continue to accept writes and serve stale data. They sync up when the partition heals.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Use Case&lt;/strong&gt;: &lt;strong&gt;Social Media&lt;/strong&gt;, &lt;strong&gt;Shopping Carts&lt;/strong&gt;. If Instagram cannot sync your latest photo globally, it will still show your profile. It stays Available, even if Inconsistent.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre class=&#34;mermaid&#34;&gt;&#xA;  graph TD&#xA;    subgraph CP [CP System - Banking]&#xA;        A[ATM 1] -. X .- B[Bank Database]&#xA;        linkStyle 0 stroke-width:2px,fill:none,stroke:red;&#xA;        User1[User] --&amp;gt; A&#xA;        A -- &amp;#34;Service Unavailable&amp;#34; --&amp;gt; User1&#xA;    end&#xA;    &#xA;    subgraph AP [AP System - Social Media]&#xA;        C[Server 1] -. X .- D[Server 2]&#xA;        linkStyle 3 stroke-width:2px,fill:none,stroke:red;&#xA;        User2[User] --&amp;gt; C&#xA;        C -- &amp;#34;Here is older data&amp;#34; --&amp;gt; User2&#xA;        %% Note: C Accepts Writes locally&#xA;    end&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;h2 id=&#34;summary-table&#34;&gt;Summary Table&#xA;&lt;/h2&gt;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Choice&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Trade-off&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Example&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;CA&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;No Partition Tolerance&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;RDBMS (MySQL)&lt;/strong&gt; (Single Node)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;CP&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Sacrifice Availability&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;MongoDB, HMAC&lt;/strong&gt; (Banking)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;AP&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Sacrifice Consistency&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Cassandra, DynamoDB&lt;/strong&gt; (Social Media)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&#xA;&lt;/h2&gt;&lt;p&gt;The CAP Theorem forces you to decide what matters more to your business.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;If losing a transaction means legal trouble (Money), choose &lt;strong&gt;CP&lt;/strong&gt;.&lt;/li&gt;&#xA;&lt;li&gt;If downtime means losing customers (Retail/Social), choose &lt;strong&gt;AP&lt;/strong&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;There is no &amp;ldquo;best&amp;rdquo; architecture, only the right one for your specific problem.&lt;/p&gt;&#xA;</description>
        </item><item>
            <title>Throughput in Distributed Systems: The Impact of Scale</title>
            <link>http://localhost:55562/blogs/system-design/throughput-in-distributed-systems/</link>
            <pubDate>Sun, 08 Feb 2026 00:00:00 +0000</pubDate>
            <guid>http://localhost:55562/blogs/system-design/throughput-in-distributed-systems/</guid>
            <description>&lt;p&gt;In our previous discussions, we covered &lt;a class=&#34;link&#34; href=&#34;http://localhost:55562/blogs/system-design/monolithic-architecture/&#34; &gt;Monoliths&lt;/a&gt;, &lt;a class=&#34;link&#34; href=&#34;http://localhost:55562/blogs/system-design/distributed-systems/&#34; &gt;Distributed Systems&lt;/a&gt;, and &lt;a class=&#34;link&#34; href=&#34;http://localhost:55562/blogs/system-design/latency-in-web-applications/&#34; &gt;Latency&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;Today, we decode &lt;strong&gt;Throughput&lt;/strong&gt; (often measured in bits per second, bps)—the measure of how much work your system can actually handle. While Latency is about &amp;ldquo;speed,&amp;rdquo; Throughput is about &amp;ldquo;capacity.&amp;rdquo;&lt;/p&gt;&#xA;&lt;h2 id=&#34;what-is-throughput&#34;&gt;What is Throughput?&#xA;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Throughput&lt;/strong&gt; is the amount of work (requests, data, transactions) a system can process in a given unit of time.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Latency&lt;/strong&gt;: How long does it take to move &lt;em&gt;one&lt;/em&gt; car from A to B?&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Throughput&lt;/strong&gt;: How many cars &lt;em&gt;per hour&lt;/em&gt; can move from A to B?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Ideally, you want &lt;strong&gt;High Throughput&lt;/strong&gt; and &lt;strong&gt;Low Latency&lt;/strong&gt;. However, improving one can sometimes hurt the other.&lt;/p&gt;&#xA;&lt;h2 id=&#34;throughput-monolithic-vs-distributed&#34;&gt;Throughput: Monolithic vs. Distributed&#xA;&lt;/h2&gt;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Feature&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Monolithic System&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Distributed System&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Resources&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Limited to one machine (CPU/RAM cap)&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Virtually unlimited (Add more machines)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Scaling&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Vertical (Expensive, Hard limit)&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Horizontal (Add nodes endlessly)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Throughput&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Low/Capped&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;High/Uncapped&lt;/strong&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Bottleneck&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;The single server itself&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Network bandwidth or Database&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h3 id=&#34;why-distributed-systems-win-on-throughput&#34;&gt;Why Distributed Systems Win on Throughput&#xA;&lt;/h3&gt;&lt;p&gt;In a Monolith, your throughput is hard-capped by the physical limits of the server. If your server can handle 1,000 requests/sec, that&amp;rsquo;s it.&lt;/p&gt;&#xA;&lt;p&gt;In a Distributed System, you can use &lt;strong&gt;Horizontal Scaling&lt;/strong&gt;. If one server handles 1,000 req/sec, ten servers handle 10,000 req/sec.&lt;/p&gt;&#xA;&lt;pre class=&#34;mermaid&#34;&gt;&#xA;  graph TD&#xA;    LB[Load Balancer] --&amp;gt; S1[Server 1]&#xA;    LB --&amp;gt; S2[Server 2]&#xA;    LB --&amp;gt; S3[Server 3]&#xA;    S1 --&amp;gt; DB[(Database)]&#xA;    S2 --&amp;gt; DB&#xA;    S3 --&amp;gt; DB&#xA;    style LB fill:#f9f,stroke:#333&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;By adding Servers (S1, S2, S3&amp;hellip;), you increase the total throughput of the system linearly.&lt;/em&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;what-kills-throughput&#34;&gt;What Kills Throughput?&#xA;&lt;/h2&gt;&lt;p&gt;Even in distributed systems, throughput isn&amp;rsquo;t infinite. Three main factors drag it down:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;Latency&lt;/strong&gt;: If each request takes longer to process (high latency), the server is busy for longer, reducing the number of requests it can handle per second.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Protocol Overhead&lt;/strong&gt;: Every network call involves handshakes (TCP/TLS) and headers. This &amp;ldquo;administrative&amp;rdquo; data consumes bandwidth that could be used for actual payload.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Congestion&lt;/strong&gt;: When too many requests arrive at once, queues fill up. Packets get dropped, causing retries, which further clogs the system.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;strategies-to-maximize-throughput&#34;&gt;Strategies to Maximize Throughput&#xA;&lt;/h2&gt;&lt;p&gt;To get the most out of your distributed system, you need to optimize the flow of data.&lt;/p&gt;&#xA;&lt;h3 id=&#34;1-load-balancing&#34;&gt;1. Load Balancing&#xA;&lt;/h3&gt;&lt;p&gt;A &lt;strong&gt;Load Balancer&lt;/strong&gt; sits in front of your servers and distributes traffic evenly (e.g., Round Robin). This ensures no single server is overwhelmed while others sit idle.&lt;/p&gt;&#xA;&lt;h3 id=&#34;2-caching--cdns&#34;&gt;2. Caching &amp;amp; CDNs&#xA;&lt;/h3&gt;&lt;p&gt;Serving data from a cache or &lt;a class=&#34;link&#34; href=&#34;http://localhost:55562/blogs/system-design/latency-in-web-applications/&#34; &gt;CDN&lt;/a&gt; avoids hitting the backend servers entirely.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Real-Life Example&lt;/strong&gt;: &lt;strong&gt;Netflix&lt;/strong&gt;. By serving video chunks from a local ISP server (CDN), they offload terabytes of data from their main servers, massively increasing global throughput.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;3-asynchronous-processing&#34;&gt;3. Asynchronous Processing&#xA;&lt;/h3&gt;&lt;p&gt;Instead of blocking a connection while waiting for a task to finish, use message queues (like Kafka). The system accepts the request immediately (improving throughput) and processes it in the background.&lt;/p&gt;&#xA;&lt;h2 id=&#34;real-life-example-ticket-booking-vs-video-streaming&#34;&gt;Real-Life Example: Ticket Booking vs. Video Streaming&#xA;&lt;/h2&gt;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Ticket Booking (IRCTC/Ticketmaster)&lt;/strong&gt;: Low Throughput, High Consistency requirement. The bottleneck is the database lock (selling the same seat twice).&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Video Streaming (YouTube)&lt;/strong&gt;: High Throughput requirement. The system needs to push gigabytes of data per second to millions of users. It relies heavily on CDNs and distributed storage to achieve this massive throughput.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&#xA;&lt;/h2&gt;&lt;p&gt;Throughput is the capacity of your system. While Monoliths are limited by single-machine physics, Distributed Systems unlock massive throughput via Horizontal Scaling. By managing &lt;strong&gt;Congestion&lt;/strong&gt; and using &lt;strong&gt;Load Balancers&lt;/strong&gt;, you can build systems that handle millions of users effortlessly.&lt;/p&gt;&#xA;</description>
        </item><item>
            <title>Time in Distributed Systems: Why Clocks Lie</title>
            <link>http://localhost:55562/blogs/system-design/time-in-distributed-systems/</link>
            <pubDate>Sun, 08 Feb 2026 00:00:00 +0000</pubDate>
            <guid>http://localhost:55562/blogs/system-design/time-in-distributed-systems/</guid>
            <description>&lt;p&gt;In a Monolithic system, &amp;ldquo;Time&amp;rdquo; is easy. You ask the OS for &lt;code&gt;Date.now()&lt;/code&gt;, and that is the absolute truth.&#xA;In a Distributed System, &amp;ldquo;Time&amp;rdquo; is a nightmare.&lt;/p&gt;&#xA;&lt;p&gt;Common questions like &amp;ldquo;Which event happened first?&amp;rdquo; become remarkably hard to answer.&lt;/p&gt;&#xA;&lt;h2 id=&#34;the-problem-with-physical-clocks&#34;&gt;The Problem with Physical Clocks&#xA;&lt;/h2&gt;&lt;p&gt;Distributed systems span different machines, often in different time zones (e.g., US, India, Australia). Each machine has its own &lt;strong&gt;Physical Clock&lt;/strong&gt;.&#xA;Even with NTP (Network Time Protocol), these clocks drift. One server might think it&amp;rsquo;s &lt;code&gt;10:00:00.001&lt;/code&gt; while another thinks it&amp;rsquo;s &lt;code&gt;10:00:00.005&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;This &lt;strong&gt;Clock Skew&lt;/strong&gt; means you cannot rely on timestamps to order events.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Scenario&lt;/strong&gt;: User A books a ticket at &lt;code&gt;10:00:01&lt;/code&gt; (Server US). User B books the same ticket at &lt;code&gt;10:00:02&lt;/code&gt; (Server India). If the India server&amp;rsquo;s clock is 5 seconds slow, it might record the time as &lt;code&gt;09:59:57&lt;/code&gt;, making it look like User B booked it first!&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;ideally-partial-ordering&#34;&gt;Ideally: Partial Ordering&#xA;&lt;/h2&gt;&lt;p&gt;We don&amp;rsquo;t need to know the &lt;em&gt;exact&lt;/em&gt; time. We just need to know the &lt;strong&gt;Order of Events&lt;/strong&gt;.&#xA;This is called the &lt;strong&gt;Happened-Before Relationship&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;h2 id=&#34;the-solution-lamport-logical-clocks&#34;&gt;The Solution: Lamport Logical Clocks&#xA;&lt;/h2&gt;&lt;p&gt;Leslie Lamport introduced a simple algorithm to solve this using &lt;strong&gt;Logical Counters&lt;/strong&gt; instead of real time.&lt;/p&gt;&#xA;&lt;h3 id=&#34;the-algorithm&#34;&gt;The Algorithm&#xA;&lt;/h3&gt;&lt;p&gt;Every process maintains a counter &lt;code&gt;C&lt;/code&gt;, initially &lt;code&gt;0&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;Internal Event&lt;/strong&gt;: Before executing an event, increment &lt;code&gt;C = C + 1&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Send Message&lt;/strong&gt;: Increment &lt;code&gt;C = C + 1&lt;/code&gt; and send &lt;code&gt;C&lt;/code&gt; along with the message.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Receive Message&lt;/strong&gt;: When receiving a message with counter &lt;code&gt;C_msg&lt;/code&gt;, update your local counter: &lt;code&gt;C = max(C, C_msg) + 1&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;visualizing-the-flow&#34;&gt;Visualizing the Flow&#xA;&lt;/h3&gt;&lt;p&gt;Let&amp;rsquo;s look at three processes (P1, P2, P3) interacting. Notice how P2&amp;rsquo;s clock jumps forward when it receives a message from P3.&lt;/p&gt;&#xA;&lt;pre class=&#34;mermaid&#34;&gt;&#xA;  %%{init: {&amp;#39;theme&amp;#39;: &amp;#39;dark&amp;#39;}}%%&#xA;sequenceDiagram&#xA;    participant P1&#xA;    participant P2&#xA;    participant P3&#xA;    &#xA;    Note over P1,P3: Initial State: All Clocks = 0&#xA;    &#xA;    P1-&amp;gt;&amp;gt;P1: Event A (C=1)&#xA;    P2-&amp;gt;&amp;gt;P2: Event B (C=1)&#xA;    &#xA;    P1-&amp;gt;&amp;gt;P2: Message (C=2)&#xA;    Note right of P2: P2 receives C=2.&amp;lt;br/&amp;gt;max(1, 2) + 1 = 3&#xA;    P2-&amp;gt;&amp;gt;P2: Event C (C=3)&#xA;    &#xA;    P3-&amp;gt;&amp;gt;P3: Event D (C=1)&#xA;    P3-&amp;gt;&amp;gt;P2: Message (C=2)&#xA;    Note right of P2: P2 receives C=2.&amp;lt;br/&amp;gt;max(3, 2) + 1 = 4&#xA;    P2-&amp;gt;&amp;gt;P2: Event E (C=4)&#xA;    &#xA;    P2-&amp;gt;&amp;gt;P1: Message (C=5)&#xA;    Note left of P1: P1 receives C=5.&amp;lt;br/&amp;gt;max(1, 5) + 1 = 6&#xA;    P1-&amp;gt;&amp;gt;P1: Event F (C=6)&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;h3 id=&#34;what-this-tells-us&#34;&gt;What This Tells Us&#xA;&lt;/h3&gt;&lt;ul&gt;&#xA;&lt;li&gt;If event &lt;code&gt;A&lt;/code&gt; caused event &lt;code&gt;B&lt;/code&gt;, then &lt;code&gt;Clock(A) &amp;lt; Clock(B)&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;This gives us a &lt;strong&gt;Partial Ordering&lt;/strong&gt; of events.&lt;/li&gt;&#xA;&lt;li&gt;It does &lt;strong&gt;not&lt;/strong&gt; give us real time (duration), but it ensures causality is respected.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&#xA;&lt;/h2&gt;&lt;p&gt;In Distributed Systems, &lt;strong&gt;Logical Time &amp;gt; Physical Time&lt;/strong&gt;.&#xA;Tools like &lt;strong&gt;Lamport Clocks&lt;/strong&gt; and &lt;strong&gt;Vector Clocks&lt;/strong&gt; (an immense improvement on Lamport) are the bedrock of systems like Amazon Dynamo, Cassandra, and Google Spanner. They allow us to agree on the sequence of events without needing atomic clocks in every server.&lt;/p&gt;&#xA;</description>
        </item><item>
            <title>Understanding Latency in Web Applications: Causes and Cures</title>
            <link>http://localhost:55562/blogs/system-design/latency-in-web-applications/</link>
            <pubDate>Sun, 08 Feb 2026 00:00:00 +0000</pubDate>
            <guid>http://localhost:55562/blogs/system-design/latency-in-web-applications/</guid>
            <description>&lt;p&gt;Speed is a feature. In system design, &lt;strong&gt;Latency&lt;/strong&gt; is the villain we are constantly fighting.&lt;/p&gt;&#xA;&lt;p&gt;In the previous post, we discussed &lt;a class=&#34;link&#34; href=&#34;http://localhost:55562/blogs/system-design/distributed-systems/&#34; &gt;Distributed Systems&lt;/a&gt;, which provide scalability at the cost of complexity. One of the biggest costs? &lt;strong&gt;Latency&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;p&gt;In this post, we&amp;rsquo;ll dissect what latency actually is, why modern distributed architectures often make it worse, and the three main weapons we have to fight it.&lt;/p&gt;&#xA;&lt;h2 id=&#34;what-is-latency&#34;&gt;What is Latency?&#xA;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Latency&lt;/strong&gt; is the time it takes for a data packet to travel from one point to another. In the context of a web application, it&amp;rsquo;s the round-trip time from the user&amp;rsquo;s action to the application&amp;rsquo;s response.&lt;/p&gt;&#xA;&lt;h3 id=&#34;the-formula-t1--t2--t3&#34;&gt;The Formula: T1 + T2 + T3&#xA;&lt;/h3&gt;&lt;p&gt;We can break down latency into three distinct phases:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;T1 (Network Delay - Request)&lt;/strong&gt;: Time for the request to travel from User → Server.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;T2 (Processing Delay - Computer)&lt;/strong&gt;: Time the server takes to think and process the request.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;T3 (Network Delay - Response)&lt;/strong&gt;: Time for the response to travel from Server → User.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;[ \text{Total Latency} = T1 + T2 + T3 ]&lt;/p&gt;&#xA;&lt;pre class=&#34;mermaid&#34;&gt;&#xA;  sequenceDiagram&#xA;    participant User&#xA;    participant Server&#xA;    Note over User, Server: T1 (Network Delay)&#xA;    User-&amp;gt;&amp;gt;Server: Request&#xA;    Note over Server: T2 (Processing)&#xA;    Note over Server, User: T3 (Network Delay)&#xA;    Server--&amp;gt;&amp;gt;User: Response&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;h2 id=&#34;architecture-impact-monolith-vs-distributed&#34;&gt;Architecture Impact: Monolith vs. Distributed&#xA;&lt;/h2&gt;&lt;p&gt;You might think that upgrading to a &amp;ldquo;modern&amp;rdquo; Distributed System (microservices) would make your app faster. &lt;strong&gt;Often, it does the opposite.&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;monolithic-architecture-low-latency&#34;&gt;Monolithic Architecture (Low Latency)&#xA;&lt;/h3&gt;&lt;p&gt;In a monolith, function calls are strictly &lt;strong&gt;in-memory&lt;/strong&gt;. When the &lt;code&gt;OrderService&lt;/code&gt; needs to check the &lt;code&gt;InventoryService&lt;/code&gt;, it&amp;rsquo;s just a function call.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Latency Profile&lt;/strong&gt;: Extremely low network overhead. Mostly T2 (Processing).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;distributed-architecture-high-latency&#34;&gt;Distributed Architecture (High Latency)&#xA;&lt;/h3&gt;&lt;p&gt;In a distributed system, that same check is now an &lt;strong&gt;HTTP request over the network&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Latency Profile&lt;/strong&gt;: Every internal communication adds a new T1 + T3 round trip.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Architecture&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Processing Delay (T2)&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Network Delay (T1 + T3)&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Overall Latency&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Monolithic&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Standard&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Minimal (External only)&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Low&lt;/strong&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Distributed&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Standard&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;High (Internal + External)&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;High&lt;/strong&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h2 id=&#34;how-to-reduce-latency&#34;&gt;How to Reduce Latency&#xA;&lt;/h2&gt;&lt;p&gt;Since we can&amp;rsquo;t change the speed of Network (T1/T3) or make CPUs infinitely fast (T2), we use architectural patterns.&lt;/p&gt;&#xA;&lt;h3 id=&#34;1-caching-reducing-t2&#34;&gt;1. Caching (Reducing T2)&#xA;&lt;/h3&gt;&lt;p&gt;If your application takes 500ms to calculate a report (T2), doing it every time is wasteful. &lt;strong&gt;Caching&lt;/strong&gt; stores the result so the next request takes 5ms.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Real-Life Example&lt;/strong&gt;: &lt;strong&gt;Twitter/X&lt;/strong&gt;. When you load your timeline, Twitter doesn&amp;rsquo;t query the database for every tweet. It pulls a pre-computed list from a Redis cache, making the load time instant.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre class=&#34;mermaid&#34;&gt;&#xA;  graph LR&#xA;    User --&amp;gt; Cache&#xA;    Cache -- Hit --&amp;gt; User&#xA;    Cache -- Miss --&amp;gt; Server&#xA;    Server --&amp;gt; Database&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;h3 id=&#34;2-content-delivery-network---cdn-reducing-t1--t3&#34;&gt;2. Content Delivery Network - CDN (Reducing T1 + T3)&#xA;&lt;/h3&gt;&lt;p&gt;If your server is in New York and your user is in Tokyo, T1 and T3 will be high because of physical distance. A &lt;strong&gt;CDN&lt;/strong&gt; stores copies of your static files (images, CSS, JS) on servers all over the world.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Real-Life Example&lt;/strong&gt;: &lt;strong&gt;Instagram&lt;/strong&gt;. The photos you see aren&amp;rsquo;t coming from a main server in the US; they are being served from a CDN edge location in your own city.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;3-hardware-scaling-reducing-t2&#34;&gt;3. Hardware Scaling (Reducing T2)&#xA;&lt;/h3&gt;&lt;p&gt;Sometimes, the simplest solution is brute force. upgrading the CPU, RAM, or using faster SSDs can significantly reduce processing time.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Benefit&lt;/strong&gt;: Easy to implement.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Downside&lt;/strong&gt;: Expensive and has limits (Vertical Scaling limit).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&#xA;&lt;/h2&gt;&lt;p&gt;Latency is the silent killer of user experience. While distributed systems offer scalability, they introduce network latency that must be managed. By understanding the &lt;strong&gt;T1+T2+T3&lt;/strong&gt; formula and aggressively using &lt;strong&gt;Caching&lt;/strong&gt; and &lt;strong&gt;CDNs&lt;/strong&gt;, you can build systems that feel instant to your users.&lt;/p&gt;&#xA;</description>
        </item><item>
            <title>Distributed Systems: Definition, Advantages, and Challenges</title>
            <link>http://localhost:55562/blogs/system-design/distributed-systems/</link>
            <pubDate>Sat, 07 Feb 2026 00:00:00 +0000</pubDate>
            <guid>http://localhost:55562/blogs/system-design/distributed-systems/</guid>
            <description>&lt;p&gt;In the previous post, we explored &lt;a class=&#34;link&#34; href=&#34;http://localhost:55562/blogs/understanding-monolithic-architecture-the-all-in-one-approach/&#34; &gt;Monolithic Architecture&lt;/a&gt;, where everything lives in one big box. While simple, monoliths struggle to scale beyond a certain point.&lt;/p&gt;&#xA;&lt;p&gt;Enter &lt;strong&gt;Distributed Systems&lt;/strong&gt;—the architecture that powers the modern internet. From Google Search to Netflix, distributed systems are what allow applications to handle millions of concurrent users without crashing.&lt;/p&gt;&#xA;&lt;h2 id=&#34;what-is-a-distributed-system&#34;&gt;What is a Distributed System?&#xA;&lt;/h2&gt;&lt;p&gt;A &lt;strong&gt;Distributed System&lt;/strong&gt; is a collection of independent computers that appear to its users as a single coherent system.&lt;/p&gt;&#xA;&lt;p&gt;Instead of one massive server doing everything, you have multiple machines (nodes) communicating over a network to achieve a common goal.&lt;/p&gt;&#xA;&lt;h3 id=&#34;monolithic-vs-distributed&#34;&gt;Monolithic vs. Distributed&#xA;&lt;/h3&gt;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Feature&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Monolithic System&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Distributed System&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Deployment&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Single server/location&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Multiple machines across networks&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Scaling&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Vertical (Add more RAM/CPU to one machine)&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Horizontal (Add more machines)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Failure&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Single Point of Failure (If server dies, app dies)&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Fault Tolerant (If one node dies, others take over)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Complexity&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Low&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;High&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h2 id=&#34;key-advantages&#34;&gt;Key Advantages&#xA;&lt;/h2&gt;&lt;h3 id=&#34;1-fault-tolerance-no-single-point-of-failure&#34;&gt;1. Fault Tolerance (No Single Point of Failure)&#xA;&lt;/h3&gt;&lt;p&gt;In a monolith, if the server crashes, your business stops. In a distributed system, data and services are &lt;strong&gt;replicated&lt;/strong&gt; across multiple machines.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Real-Life Example&lt;/strong&gt;: If one Google data center goes offline due to a power outage, your search query is simply routed to another data center. You don&amp;rsquo;t even notice the failure.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;2-horizontal-scalability&#34;&gt;2. Horizontal Scalability&#xA;&lt;/h3&gt;&lt;p&gt;This is the superpower of distributed systems.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Vertical Scaling (Monolith)&lt;/strong&gt;: Upgrading a server is like buying a bigger truck. Eventually, you can&amp;rsquo;t buy a bigger truck.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Horizontal Scaling (Distributed)&lt;/strong&gt;: Simply buy &lt;em&gt;more&lt;/em&gt; trucks. You can add cheap, commodity hardware to the cluster to handle increased load endlessly.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;3-low-latency-geo-distribution&#34;&gt;3. Low Latency (Geo-Distribution)&#xA;&lt;/h3&gt;&lt;p&gt;You can place servers closer to your users.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Real-Life Example&lt;/strong&gt;: &lt;strong&gt;Netflix&lt;/strong&gt;. When you stream a movie, you aren&amp;rsquo;t downloading it from Netflix HQ in California. You are streaming it from an Open Connect appliance (CDN) located at your local ISP&amp;rsquo;s data center, ensuring instant buffering.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;challenges-the-price-of-power&#34;&gt;Challenges: The Price of Power&#xA;&lt;/h2&gt;&lt;p&gt;Distributed systems are powerful, but they are hard to build and manage.&lt;/p&gt;&#xA;&lt;h3 id=&#34;1-data-consistency&#34;&gt;1. Data Consistency&#xA;&lt;/h3&gt;&lt;p&gt;If you have data on five different machines, how do you ensure they all agree?&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Scenario&lt;/strong&gt;: User A updates their profile picture on Node 1. User B views the profile on Node 2. If replication is slow, User B sees the old photo. This is the classic &lt;strong&gt;Consistency vs. Availability&lt;/strong&gt; trade-off (CAP Theorem).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;2-network-unreliability&#34;&gt;2. Network Unreliability&#xA;&lt;/h3&gt;&lt;p&gt;Calls within a monolith are instant function calls. Calls in a distributed system go over a network. Networks are unreliable—packets get lost, latency spikes, and connections drop.&lt;/p&gt;&#xA;&lt;h3 id=&#34;3-operational-complexity&#34;&gt;3. Operational Complexity&#xA;&lt;/h3&gt;&lt;p&gt;Managing one server is easy. Managing a fleet of 1,000 servers requires sophisticated tools for:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Orchestration&lt;/strong&gt; (Kubernetes)&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Monitoring&lt;/strong&gt; (Prometheus, Grafana)&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Logging&lt;/strong&gt; (ELK Stack)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;real-life-example-google-search&#34;&gt;Real-Life Example: Google Search&#xA;&lt;/h2&gt;&lt;p&gt;Google Search is the ultimate distributed system.&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;Crawling&lt;/strong&gt;: Thousands of machines crawl the web in parallel.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Indexing&lt;/strong&gt;: The index is too large for one computer, so it&amp;rsquo;s &lt;strong&gt;sharded&lt;/strong&gt; across thousands of machines.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Searching&lt;/strong&gt;: When you type a query, it&amp;rsquo;s sent to hundreds of machines that search their specific shard of the index in parallel. The results are then aggregated and returned to you in milliseconds.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&#xA;&lt;/h2&gt;&lt;p&gt;Distributed Systems are the answer to &amp;ldquo;How do we scale big?&amp;rdquo; They offer resilience and infinite scaling but introduce significant complexity in data management and operations. For most startups, a Monolith is the right start. For a global enterprise, a Distributed System is a necessity.&lt;/p&gt;&#xA;</description>
        </item><item>
            <title>Introduction to System Design: Concepts, Process, and Types</title>
            <link>http://localhost:55562/blogs/system-design/introduction-to-system-design/</link>
            <pubDate>Sat, 07 Feb 2026 00:00:00 +0000</pubDate>
            <guid>http://localhost:55562/blogs/system-design/introduction-to-system-design/</guid>
            <description>&lt;p&gt;System Design is one of the most critical skills for software engineers, especially as they progress to senior roles. It’s the process of defining the architecture, modules, interfaces, and data for a system to satisfy specified requirements.&lt;/p&gt;&#xA;&lt;p&gt;In simple terms, if coding is about &lt;em&gt;how&lt;/em&gt; to write a function, System Design is about &lt;em&gt;where&lt;/em&gt; that function lives, how it talks to other parts of the application, and how the entire system scales to handle millions of users.&lt;/p&gt;&#xA;&lt;h2 id=&#34;what-is-system-design&#34;&gt;What is System Design?&#xA;&lt;/h2&gt;&lt;p&gt;System Design is the process of defining the elements of a system—such as architecture, modules, and components—and their interfaces and data for a system to satisfy specified requirements. It is the bridge between a problem statement and the final code.&lt;/p&gt;&#xA;&lt;p&gt;The main objective is to &lt;strong&gt;define and organize the elements&lt;/strong&gt; of an application to ensure that all parts work cohesively. A well-designed system is:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Scalable&lt;/strong&gt;: Can handle growth in users and data.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Reliable&lt;/strong&gt;: Functions correctly even when components fail.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Maintainable&lt;/strong&gt;: Easy to understand and modify.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;types-of-system-design&#34;&gt;Types of System Design&#xA;&lt;/h2&gt;&lt;p&gt;In the context of software engineering, System Design is generally categorized into two main types: &lt;strong&gt;High-Level Design (HLD)&lt;/strong&gt; and &lt;strong&gt;Low-Level Design (LLD)&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;h3 id=&#34;1-high-level-design-hld&#34;&gt;1. High-Level Design (HLD)&#xA;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;&amp;ldquo;The Big Picture&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;HLD focuses on the overall system architecture. It identifies the major components of the system and how they interact with each other. It answers questions like:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&amp;ldquo;Microservices or Monolith?&amp;rdquo;&lt;/li&gt;&#xA;&lt;li&gt;&amp;ldquo;SQL or NoSQL?&amp;rdquo;&lt;/li&gt;&#xA;&lt;li&gt;&amp;ldquo;How do we handle caching?&amp;rdquo;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;strong&gt;Real-Life Example: Designing Uber (HLD)&lt;/strong&gt;&#xA;When designing the HLD for Uber, you wouldn&amp;rsquo;t worry about the specific class for a &lt;code&gt;Driver&lt;/code&gt;. Instead, you&amp;rsquo;d define:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Load Balancers&lt;/strong&gt; to distribute traffic.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Microservices&lt;/strong&gt; for User Management, Ride Matching, and Payments.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Database&lt;/strong&gt; choices (e.g., PostgreSQL for transactions, Cassandra for location history).&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Communication Protocols&lt;/strong&gt; (e.g., WebSockets for real-time location updates).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;2-low-level-design-lld&#34;&gt;2. Low-Level Design (LLD)&#xA;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;&amp;ldquo;The Detailed View&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;LLD dives into the implementation details of the components defined in the HLD. It focuses on the internal logic of modules, data structures, algorithms, and class diagrams.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Real-Life Example: Designing the &lt;code&gt;RideMatching&lt;/code&gt; Service (LLD)&lt;/strong&gt;&#xA;Continuing with the Uber example, LLD would define:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Class Diagrams&lt;/strong&gt;: A &lt;code&gt;Driver&lt;/code&gt; class, a &lt;code&gt;Rider&lt;/code&gt; class, and a &lt;code&gt;Trip&lt;/code&gt; class.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Interfaces&lt;/strong&gt;: Public methods like &lt;code&gt;findNearestDriver(location)&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Database Schema&lt;/strong&gt;: The exact columns in the &lt;code&gt;Trips&lt;/code&gt; table.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Algorithms&lt;/strong&gt;: The specific logic used to match a rider with a driver (e.g., using a QuadTree for geospatial search).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;the-system-design-process&#34;&gt;The System Design Process&#xA;&lt;/h2&gt;&lt;p&gt;Designing a system is not a linear process, but it generally follows a structured flow:&lt;/p&gt;&#xA;&lt;h3 id=&#34;step-1-requirements-gathering&#34;&gt;Step 1: Requirements Gathering&#xA;&lt;/h3&gt;&lt;p&gt;Before drawing any diagrams, you must understand what you are building.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Functional Requirements&lt;/strong&gt;: What should the system do? (e.g., &amp;ldquo;Users can post tweets&amp;rdquo;).&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Non-Functional Requirements&lt;/strong&gt;: How should the system behave? (e.g., &amp;ldquo;The system must be highly available&amp;rdquo;).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;step-2-define-architecture-hld&#34;&gt;Step 2: Define Architecture (HLD)&#xA;&lt;/h3&gt;&lt;p&gt;Decide on the high-level structure.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Monolith vs. Microservices&lt;/strong&gt;: Choose based on team size and complexity.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Data Flow&lt;/strong&gt;: How does data move from the user to the database?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;step-3-identify-modules&#34;&gt;Step 3: Identify Modules&#xA;&lt;/h3&gt;&lt;p&gt;Break the system down into smaller, manageable chunks.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;For an E-commerce site: Inventory Service, User Service, Cart Service, Order Service.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;step-4-define-interaction-points&#34;&gt;Step 4: Define Interaction Points&#xA;&lt;/h3&gt;&lt;p&gt;Specify how these modules will communicate.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Synchronous&lt;/strong&gt;: REST API or gRPC.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Asynchronous&lt;/strong&gt;: Message Queues (Kafka, RabbitMQ).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;step-5-data-modeling-and-logic-lld&#34;&gt;Step 5: Data Modeling and Logic (LLD)&#xA;&lt;/h3&gt;&lt;p&gt;Define the database schema and business logic.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Schema Design&lt;/strong&gt;: Tables, relationships, and indexes.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;API Design&lt;/strong&gt;: Define the endpoints (e.g., &lt;code&gt;POST /api/v1/checkout&lt;/code&gt;).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;summary&#34;&gt;Summary&#xA;&lt;/h2&gt;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Feature&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;High-Level Design (HLD)&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Low-Level Design (LLD)&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Focus&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Overall Architecture&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Component Implementation&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Audience&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Architects, Stakeholders&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Developers&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Deliverables&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;System Diagrams, Tech Stack&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Class Diagrams, DB Schema&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&amp;ldquo;Use Redis for caching&amp;rdquo;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&amp;ldquo;Use &lt;code&gt;LRUCache&lt;/code&gt; class with &lt;code&gt;HashMap&lt;/code&gt;&amp;rdquo;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&#xA;&lt;/h2&gt;&lt;p&gt;System Design is finding the optimal solution to a problem under constraints. Whether you are building a small startup app or a global platform like Netflix, understanding the distinction between HLD and LLD and following a structured design process is key to success.&lt;/p&gt;&#xA;&lt;p&gt;Stay tuned for more deep dives into specific System Design components like &lt;strong&gt;Load Balancing&lt;/strong&gt;, &lt;strong&gt;Caching strategies&lt;/strong&gt;, and &lt;strong&gt;Database Sharding&lt;/strong&gt;!&lt;/p&gt;&#xA;</description>
        </item><item>
            <title>Understanding Monolithic Architecture: The &#39;All-in-One&#39; Approach</title>
            <link>http://localhost:55562/blogs/system-design/monolithic-architecture/</link>
            <pubDate>Sat, 07 Feb 2026 00:00:00 +0000</pubDate>
            <guid>http://localhost:55562/blogs/system-design/monolithic-architecture/</guid>
            <description>&lt;p&gt;In the world of software architecture, &amp;ldquo;Monolithic&amp;rdquo; often gets a bad rap, associated with &amp;ldquo;legacy&amp;rdquo; or &amp;ldquo;old-school&amp;rdquo; code. But the truth is, &lt;strong&gt;Monolithic Architecture&lt;/strong&gt; is the foundation upon which many of today&amp;rsquo;s tech giants were built. It remains the most practical choice for many new projects.&lt;/p&gt;&#xA;&lt;p&gt;This post breaks down what Monolithic Architecture really is, its core components, and why you might—or might not—want to use it.&lt;/p&gt;&#xA;&lt;h2 id=&#34;what-is-monolithic-architecture&#34;&gt;What is Monolithic Architecture?&#xA;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Monolithic Architecture&lt;/strong&gt; is a unified model for designing a software application. In this approach, the application is built as a &lt;strong&gt;single, indivisible unit&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;p&gt;Think of it like a massive Lego castle where every brick is glued together. You can&amp;rsquo;t just take off a tower and replace it without potentially affecting the stability of the entire structure.&lt;/p&gt;&#xA;&lt;h3 id=&#34;the-unified-concept&#34;&gt;The &amp;ldquo;Unified&amp;rdquo; Concept&#xA;&lt;/h3&gt;&lt;p&gt;In a typical web application, you have three main layers:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;Frontend&lt;/strong&gt;: The User Interface (UI).&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Backend&lt;/strong&gt;: The Business Logic.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Data Layer&lt;/strong&gt;: The Database interface.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;In a Monolith, all these layers are managed within a &lt;strong&gt;single codebase&lt;/strong&gt; and deployed as a &lt;strong&gt;single executable&lt;/strong&gt;. Even if the database itself runs on a separate server (like a managed PostgreSQL instance), the &lt;em&gt;code&lt;/em&gt; that interacts with it is tightly bundled with the business logic and API endpoints.&lt;/p&gt;&#xA;&lt;h2 id=&#34;characteristics-of-a-monolith&#34;&gt;Characteristics of a Monolith&#xA;&lt;/h2&gt;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;Single Codebase&lt;/strong&gt;: All features (User Auth, Payments, Inventory) live in one Git repository.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Unified Deployment&lt;/strong&gt;: To update one line of code, you must redeploy the entire application.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Shared Memory&lt;/strong&gt;: Components can call each other directly (function calls) rather than over a network (API calls), making communication extremely fast.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;advantages-why-start-with-a-monolith&#34;&gt;Advantages: Why Start with a Monolith?&#xA;&lt;/h2&gt;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Simplicity&lt;/strong&gt;: It&amp;rsquo;s much easier to develop, test, and deploy one application than twenty microservices. &amp;ldquo;git push heroku master&amp;rdquo; is the epitome of Monolithic simplicity.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Lower Latency&lt;/strong&gt;: Communication between modules happens in-memory. There&amp;rsquo;s no network overhead of serializing JSON and making HTTP requests between services.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Easier Debugging&lt;/strong&gt;: You can trace a request from start to finish in a single IDE window. No need for complex distributed tracing tools.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Consistency&lt;/strong&gt;: It&amp;rsquo;s easier to enforce code standards and transactional integrity (ACID) when everything shares the same database.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;disadvantages-the-growing-pains&#34;&gt;Disadvantages: The Growing Pains&#xA;&lt;/h2&gt;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Tight Coupling&lt;/strong&gt;: A bug in the &amp;ldquo;Recommendations&amp;rdquo; module could crash the entire application, including &amp;ldquo;Checkout&amp;rdquo;.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Scalability Issues&lt;/strong&gt;: You can&amp;rsquo;t scale just the &amp;ldquo;Video Processing&amp;rdquo; part of your app. You have to scale the &lt;em&gt;entire&lt;/em&gt; monolith, which wastes resources.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;&amp;ldquo;Dependency Hell&amp;rdquo;&lt;/strong&gt;: Updating a library for one module might break another module that relies on an older version.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Slow Build &amp;amp; Deploy&lt;/strong&gt;: As the codebase grows to millions of lines, compiling and deploying can take upwards of 30-40 minutes.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;real-life-examples&#34;&gt;Real-Life Examples&#xA;&lt;/h2&gt;&lt;h3 id=&#34;1-the-startup-mvp-most-common&#34;&gt;1. The Startup MVP (Most Common)&#xA;&lt;/h3&gt;&lt;p&gt;Almost every successful startup—&lt;strong&gt;Airbnb, Uber, Instagram&lt;/strong&gt;—started as a Monolith.&#xA;Why? Because when you are validating a product, speed of iteration is everything. The overhead of managing microservices slows you down.&#xA;&lt;em&gt;Example&lt;/em&gt;: A simple E-commerce store built with &lt;strong&gt;Ruby on Rails&lt;/strong&gt; or &lt;strong&gt;Django&lt;/strong&gt;. The storefront, admin panel, and payment processing are all in one app.&lt;/p&gt;&#xA;&lt;h3 id=&#34;2-stack-overflow-the-majestic-monolith&#34;&gt;2. Stack Overflow (The Majestic Monolith)&#xA;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Stack Overflow&lt;/strong&gt;, one of the highest-traffic sites on the internet, runs on a monolithic architecture (.NET). They prove that with proper optimization (caching, efficient DB queries), a monolith can scale to handle billions of requests. They didn&amp;rsquo;t need microservices to succeed; they needed good engineering.&lt;/p&gt;&#xA;&lt;h3 id=&#34;3-shopify-modular-monolith&#34;&gt;3. Shopify (Modular Monolith)&#xA;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Shopify&lt;/strong&gt; is another giant that famously stuck with a monolith for a long time. They eventually evolved into a &lt;strong&gt;Modular Monolith&lt;/strong&gt;—where the code is still in one repo and deploys together, but the internal boundaries are strictly enforced to prevent &amp;ldquo;spaghetti code.&amp;rdquo;&lt;/p&gt;&#xA;&lt;h2 id=&#34;conclusion-monolith-or-not&#34;&gt;Conclusion: Monolith or Not?&#xA;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Choose a Monolith if:&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;You are a startup or building an MVP.&lt;/li&gt;&#xA;&lt;li&gt;Your team is small (e.g., &amp;lt; 10 developers).&lt;/li&gt;&#xA;&lt;li&gt;You want to focus on business features, not infrastructure complexity.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;strong&gt;Consider Distributed/Microservices if:&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;You have large, independent teams that need to deploy separately.&lt;/li&gt;&#xA;&lt;li&gt;Specific parts of your app need independent scaling (e.g., video encoding).&lt;/li&gt;&#xA;&lt;li&gt;The monolith has become too large to build and test efficiently.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Monolithic Architecture is not &amp;ldquo;outdated&amp;rdquo;—it&amp;rsquo;s a valid, powerful design choice. The key is knowing when it fits your needs and when it&amp;rsquo;s time to break it apart.&lt;/p&gt;&#xA;</description>
        </item></channel>
</rss>
