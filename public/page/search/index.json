[{"content":"Imagine it\u0026rsquo;s the day of your board exam results. You, along with millions of other students, log in to the result portal at 10:00 AM. The site crashes. That is an Availability failure.\nIn previous posts, we discussed Throughput and Latency. Today, we tackle the most critical aspect of any production system: staying online.\nCore Concepts 1. Availability Availability is the percentage of time a system is operational and accessible to users. It\u0026rsquo;s often measured in \u0026ldquo;nines\u0026rdquo;:\n99%: Down for 3.65 days/year. 99.99% (\u0026ldquo;Four Nines\u0026rdquo;): Down for 52 minutes/year. 99.999% (\u0026ldquo;Five Nines\u0026rdquo;): Down for 5 minutes/year. 2. Fault Tolerance Fault Tolerance is the ability of a system to continue operating properly in the event of the failure of some of its components.\nExample: If one engine of an airplane fails, the plane can still fly. The plane is Fault Tolerant. 3. Single Point of Failure (SPOF) A part of a system that, if it fails, stops the entire system from working.\nMonoliths vs. Distributed Systems The architectural choice you make dictates your system\u0026rsquo;s survival strategy.\nFeature Monolithic Architecture Distributed System Structure All-in-one bundle Modular, spread across nodes Failure Mode SPOF: If the server crashes, everything dies. Resilient: If one node dies, others take over. Availability Low (Requires downtime for updates/crashes) High (Zero-downtime deployments) Recovery Reboot the entire beast Automatic failover graph TD subgraph Monolith [Monolithic System - SPOF] User1[User] --\u0026gt; Server[Single Server] Server -.-\u0026gt; Crash[X Crash] Crash -.-\u0026gt; Down[System Offline] style Server fill:#ff9999 end subgraph Distributed [Distributed System - Fault Tolerant] User2[User] --\u0026gt; LB[Load Balancer] LB --\u0026gt; NodeA[Node A] LB --\u0026gt; NodeB[Node B] NodeA -.-\u0026gt; Fail[X Fail] LB ==Failover==\u0026gt; NodeB style NodeA fill:#ff9999 style NodeB fill:#99ff99 end The Secret Sauce: Replication How do distributed systems achieve high availability? Redundancy.\nWe don\u0026rsquo;t just rely on one server. We Replicate everything.\nApplication Replication: Run the same code on 10 different servers. If 3 crash, 7 are still running. Data Replication: Store your user data on a primary database and sync it to a standby replica. If the primary burns down, the standby takes over. Geographic Replication: Don\u0026rsquo;t put all servers in one data center. If the entire underlying power grid of a region fails, your app typically keeps running from a different region. The ACID Trade-off In databases, replication introduces complexity. If you write data to Node A, it takes time to copy to Node B. This touches on the CAP Theorem (Consistency vs. Availability), which we will cover in depth later.\nReal-Life Examples 1. Amazon Shopping (Prime Day) On Prime Day, traffic spikes 100x. Amazon uses thousands of microservices distributed across the globe. If the \u0026ldquo;Reviews\u0026rdquo; service crashes, you can still buy items. The system degrades gracefully rather than failing completely.\n2. Google Search Google indexes the web across thousands of machines. If the specific server holding the index for \u0026ldquo;SpaceX\u0026rdquo; fails, a replica immediately answers your query. You, the user, never know a failure occurred.\nConclusion Monoliths put all eggs in one basket. If that basket drops, you have a mess. Distributed Systems accept that failure is inevitable. Hard drives die, networks cut out, and power fails. By designing with Fault Tolerance and Replication in mind, we build systems that can survive the chaos of the real world.\n","date":"2026-02-08T00:00:00Z","permalink":"/blogs/system-design/availability-and-fault-tolerance/","title":"Availability \u0026 Fault Tolerance: Keeping the Lights On"},{"content":"Cache memory (RAM) is fast but limited. Eventually, it gets full. When you need to store new data but the cache is full, you must delete old data. This is Cache Eviction.\nThe Concept: Time-to-Live (TTL) Even if the cache isn\u0026rsquo;t full, data shouldn\u0026rsquo;t live forever. Time-to-Live (TTL) is a setting that automatically deletes data after a certain period (e.g., 5 minutes, 1 hour).\nThe Netflix Example Netflix used to cache subscription plans with a 10-year TTL.\nProblem: When they changed from 4 plans to 5 plans, users still saw the old 4 plans because the cache never expired. Fix: They reduced the TTL to hours/days to ensure users see updates. Eviction Algorithms When the cache is full, how do we decide who gets kicked out?\n1. Least Recently Used (LRU) \u0026ldquo;If you haven\u0026rsquo;t used it in a while, you probably won\u0026rsquo;t use it soon.\u0026rdquo;\nHow it works: Removes the item that was accessed longest ago. Use Case: Most common strategy. Good for social media feeds (recent posts are hot, old posts are cold). %%{init: {\u0026#39;theme\u0026#39;: \u0026#39;dark\u0026#39;}}%% graph LR subgraph Cache [\u0026#34;LRU Cache (Size 3)\u0026#34;] A[\u0026#34;A (Oldest)\u0026#34;] --\u0026gt; B[\u0026#34;B\u0026#34;] B --\u0026gt; C[\u0026#34;C (Newest)\u0026#34;] end New[\u0026#34;New Item D\u0026#34;] -.-\u0026gt;|Evicts A| A style A fill:#ff9999,stroke:#333 style New fill:#99ff99,stroke:#333 2. Least Frequently Used (LFU) \u0026ldquo;If you rarely use it, get rid of it.\u0026rdquo;\nHow it works: Tracks how often an item is accessed. Removes the one with the lowest count. Use Case: Netflix (Bollywood movies might be less watched than Hollywood blockbusters globally, so Bollywood gets evicted first in US servers). 3. First In First Out (FIFO) \u0026ldquo;Oldest data leaves first.\u0026rdquo;\nHow it works: A simple queue. First item added is the first one deleted. Use Case: Time-series data logs. 4. Most Recently Used (MRU) \u0026ldquo;Delete the newest item.\u0026rdquo;\nUse Case: Rare. Example: \u0026ldquo;Tinder\u0026rdquo; (you don\u0026rsquo;t want to see the same person you just swiped on again). 5. Random Replacement \u0026ldquo;Just delete something.\u0026rdquo;\nUse Case: When you don\u0026rsquo;t care or want to avoid overhead. Strategy Comparison Strategy Description Best For LRU Remove oldest access General purpose, Social Media LFU Remove lowest frequency Content Distribution (Netflix, YouTube) FIFO Remove oldest insertion Logs, Queues MRU Remove newest access Specific looping scenarios Random Remove random item Low overhead needed Conclusion There is no \u0026ldquo;perfect\u0026rdquo; strategy.\nFor most web apps, LRU is the safe default. For content platforms (Netflix), LFU might be better to keep popular content hot. ","date":"2026-02-08T00:00:00Z","permalink":"/blogs/system-design/cache-eviction-strategies/","title":"Cache Eviction Strategies: What to Delete?"},{"content":"Latency kills user experience. If your app takes 3 seconds to load, users leave. The database is often the bottleneck. The solution? Don\u0026rsquo;t ask the database.\nWhat is Caching? Caching is storing frequently accessed data in a temporary storage location (usually RAM) so that future requests can be served faster.\nDatabase (Disk): Slow. Doing a complex SQL join takes time. Cache (RAM): Fast. Reading from memory takes microseconds. How it Works: Cache Hit vs. Miss Request: App asks for User:123. Cache Check: Is User:123 in Redis? Yes (Cache Hit): Return data immediately. Fast. No (Cache Miss): Fetch from DB, save to Cache, then return data. Slow (once). %%{init: {\u0026#39;theme\u0026#39;: \u0026#39;dark\u0026#39;}}%% graph TD App[Application] --\u0026gt; Check{In Cache?} Check -- Yes --\u0026gt; Return[Return Data] Check -- No --\u0026gt; DB[Fetch from DB] DB --\u0026gt; Save[Save to Cache] Save --\u0026gt; Return style Check fill:#ffcc00,stroke:#333 style Return fill:#99ff99,stroke:#333 Real-World Examples Netflix: Movie metadata (descriptions, actors) rarely changes. It\u0026rsquo;s cached globally. Instagram: Your profile info (bio, follower count) is cached. When you edit your profile, the cache is invalidated (cleared). Google: Search results for common queries (\u0026ldquo;Facebook\u0026rdquo;, \u0026ldquo;Weather\u0026rdquo;) are served from cache. Types of Caching 1. In-Memory Cache (Local) Stored on the server\u0026rsquo;s own RAM.\nPros: Fastest (no network call). Cons: If server restarts, data is lost. Not shared between servers. Tool: Guava, Caffeine. 2. Distributed Cache (Shared) A separate cluster of servers dedicated to caching.\nPros: Shared across all application servers. Persistent. Scalable. Cons: Network call needed (slightly slower than local). Tool: Redis, Memcached. 3. Content Delivery Network (CDN) Geographically distributed servers that cache static assets (images, CSS, JS, Videos).\nPros: Serves content from a server close to the user (e.g., a London user gets data from a London server). Tool: Cloudflare, AWS CloudFront. Caching Strategy Cheat Sheet Type Best For Example Tool Local Cache Session data, heavy computations Caffeine Distributed Cache Database queries, API responses Redis CDN Images, Videos, HTML, CSS Cloudflare Cache Invalidation: The Hard Part Data in the database changes. If the cache still has the old data, users see stale content. How do we fix this?\nTime-to-Live (TTL): Automatically delete data after a set time (e.g., 5 minutes). Good for news feeds or weather. Manual Invalidation: Explicitly delete the cache when data changes (e.g., when a user updates their profile). Conclusion Caching is the easiest way to scale a read-heavy system.\nEffect: Reduces load on the database. Trade-off: Complexity in invalidation (\u0026ldquo;There are only two hard things in Computer Science: cache invalidation and naming things.\u0026rdquo;). ","date":"2026-02-08T00:00:00Z","permalink":"/blogs/system-design/caching/","title":"Caching: The Secret to Speed"},{"content":"In our last post on Availability, we saw that Replication is key to keeping systems alive. But Replication introduces a new problem: Consistency.\nIf you have two copies of a database, and you update one, the other is instantly \u0026ldquo;stale.\u0026rdquo; Ideally, both would update instantly, but in the real world, that takes time.\nWhat is Consistency? Consistency means that every read receives the most recent write or an error. In simple terms: All clients see the same data at the same time.\nReal-Life Example: The ATM Problem Imagine you have $500 in your bank account.\nYou withdraw $500 from an ATM in Delhi. Your Spouse simultaneously tries to withdraw $500 from an ATM in Pune. In a Consistent system, the second transaction fails because the balance is $0. In an Inconsistent system, both transactions might succeed, and the bank loses money.\nMonoliths vs. Distributed Systems System Type Consistency Level Why? Monolithic Naturally High Single database (Source of Truth). No sync needed. Distributed Hard to Maintain Data is spread across nodes. Sync takes time (Network Latency). Types of Consistency Models In distributed systems, you often have to choose between speed and accuracy.\n1. Strong Consistency Definition: Once a write is successful, all subsequent reads return the new value. Mechanism: The system locks data during updates. Read operations wait until all replicas acknowledge the update. Trade-off: Higher Latency (slower). Use Case: Banking Systems, Stock Trading, Train Ticket Booking (IRCTC). You cannot sell the same seat twice. sequenceDiagram participant Client participant Primary participant Replica Client-\u0026gt;\u0026gt;Primary: Write(X=10) Primary-\u0026gt;\u0026gt;Replica: Sync(X=10) Replica--\u0026gt;\u0026gt;Primary: Acknowledge Primary--\u0026gt;\u0026gt;Client: Success Note right of Client: Now all reads see X=10 2. Eventual Consistency Definition: If no new updates are made, eventually all accesses will return the last updated value. Mechanism: The system returns \u0026ldquo;Success\u0026rdquo; to the client immediately after updating the primary node. Replicas update in the background. Trade-off: Lower Latency (faster), but risk of stale data. Use Case: Social Media Feeds (Instagram/Twitter). If you change your profile picture, it’s okay if your friend sees the old one for a few more seconds. 3. Weak Consistency Definition: There is no guarantee that a read will return the most recent write. It relies on \u0026ldquo;best effort.\u0026rdquo; Use Case: Live Video Streaming, VoIP. If you miss a frame of video, it’s gone. The system doesn\u0026rsquo;t pause to sync. How to Improve Consistency Stop Read Operations: During a major update, put the system in \u0026ldquo;Maintenance Mode.\u0026rdquo; (Brute force approach). Reduce Replica Distance: Place replicas closer to each other to minimize sync time. Application Coordination: Use consensus algorithms like Paxos or Raft (used in Kubernetes/Etcd) to ensure nodes agree on the truth. Conclusion Consistency is a spectrum.\nNeed Accuracy? Choose Strong Consistency (but accept slower performance). Need Speed? Choose Eventual Consistency (but accept temporary staleness). As a system architect, your job is to know which part of your app needs which model. Billing must be Strong; Recommendations can be Eventual.\n","date":"2026-02-08T00:00:00Z","permalink":"/blogs/system-design/consistency-in-distributed-systems/","title":"Consistency in Distributed Systems: The Data Truth"},{"content":"When you have multiple servers (Horizontal Scaling), you need a way to distribute traffic among them. Enter the Load Balancer (LB).\nA Load Balancer acts as the entry point to your system. It accepts requests from clients and routes them to the appropriate backend server.\nCore Concepts 1. Virtual IP (VIP) Clients don\u0026rsquo;t talk to individual servers (IP1, IP2). They talk to the VIP of the Load Balancer. The LB then forwards the request internally.\n2. Health Checks The LB constantly pings your servers (\u0026ldquo;Are you alive?\u0026rdquo;). If a server crashes, the LB stops sending traffic to it. This ensures High Availability.\n3. High Availability (HA) for the LB itself If the LB crashes, your whole site goes down. To prevent this, we use Redundancy (Active-Passive LB setup).\n%%{init: {\u0026#39;theme\u0026#39;: \u0026#39;dark\u0026#39;}}%% graph TD Client --\u0026gt; VIP[Virtual IP] VIP --\u0026gt; LB1[Active LB] VIP -.-\u0026gt; LB2[Passive LB] LB1 --\u0026gt; S1[Server 1] LB1 --\u0026gt; S2[Server 2] LB1 --\u0026gt; S3[Server 3] style LB1 fill:#99ff99,stroke:#333 style LB2 fill:#ff9999,stroke:#333 Load Balancing Algorithms How does the LB decide which server gets the next request?\nAlgorithm Description Use Case Round Robin Cyclic order (S1 -\u0026gt; S2 -\u0026gt; S3 -\u0026gt; S1). Simple. Servers with equal specs. Weighted Round Robin Assigns more requests to powerful servers. Heterogeneous fleet (some big, some small servers). Least Connections Sends to the server with fewest active connections. Long-lived connections (e.g., WebSocket, Chat). Least Response Time Sends to the fastest responding server. Performance-sensitive apps. IP Hash Hashes Client IP to map to a specific server. Session Stickiness (Ensure User A always goes to Server 1). Timeline of a Request Client sends request to api.example.com (VIP). LB receives request. LB checks Health Status of backend pool. LB selects a server using an Algorithm (e.g., Round Robin). LB forwards request to Server 2. Server 2 processes and responds. Conclusion Load Balancers are essential for scaling. They provide:\nScalability: Add servers seamlessly. Availability: Automatically bypass dead servers. Flexibility: Perform maintenance on servers without downtime. However, remember that the LB itself can be a bottleneck or a single point of failure if not configured with redundancy.\n","date":"2026-02-08T00:00:00Z","permalink":"/blogs/system-design/load-balancing/","title":"Load Balancing: The Traffic Cop of Distributed Systems"},{"content":"The step up from a File-Based System is the Relational Database Management System (RDBMS). Examples: MySQL, PostgreSQL, Oracle, SQL Server.\nInstead of loose files, data is stored in Tables with strict structures.\nCore Concepts Tables: Like Excel sheets (Rows \u0026amp; Columns). Relationships: Tables are linked via Foreign Keys. Schema: The structure is fixed. You must define columns (e.g., Age is an Integer) upfront. Why RDBMS Wins (vs. Files) 1. No Redundancy Data is normalized.\nCustomer Table: Stores Name \u0026amp; Address once. Order Table: Links to Customer ID. If the customer moves, you update the address in one place. 2. Consistency \u0026amp; Integrity RDBMS enforces rules.\nConstraints: You can\u0026rsquo;t enter \u0026ldquo;Hello\u0026rdquo; into an Age column. ACID Properties: Transactions are \u0026ldquo;All or Nothing\u0026rdquo;. If money leaves Account A but fails to reach Account B, the entire transaction rolls back. 3. Powerful Querying (SQL) Instead of writing a script to open 1,000 files, you write:\n1SELECT * FROM Students WHERE GPA \u0026gt; 3.5; %%{init: {\u0026#39;theme\u0026#39;: \u0026#39;dark\u0026#39;}}%% erDiagram CUSTOMER ||--o{ ORDER : places CUSTOMER { int id PK string name string email } ORDER { int id PK int customer_id FK string product } The Limitations RDBMS is great, but it has flaws.\n1. Rigid Schema Changing the structure is hard.\nScenario: You want to add a \u0026ldquo;GitHub Profile\u0026rdquo; column to the User table. Problem: You have to run an ALTER TABLE command, which can lock the database and cause downtime for millions of rows. 2. Scalability Issues RDBMS is designed for Vertical Scaling (Bigger Server). Horizontal Scaling (Sharding) is very difficult because relationships (JOINs) heavily limit how you can split data across servers.\nComparison Feature File System RDBMS Structure Unstructured Files Structured Tables Redundancy High (Duplicate data) Low (Normalization) Consistency Risk of mismatch Enforced (ACID) Querying Manual Parsing SQL Scaling Hard Vertical (Good), Horizontal (Hard) Conclusion RDBMS is the industry standard for a reason. It provides safety and structure. However, when you need massive scale or flexible data structures, you might hit a wall. That\u0026rsquo;s where NoSQL comes in (stay tuned!).\n","date":"2026-02-08T00:00:00Z","permalink":"/blogs/system-design/rdbms/","title":"RDBMS: Bringing Order to Chaos"},{"content":"These two terms are often used interchangeably, but in System Design, they mean very different things.\nRedundancy: \u0026ldquo;I have a spare tire.\u0026rdquo; Replication: \u0026ldquo;I have a spare tire, and it has the exact same air pressure and tread wear as the main tire.\u0026rdquo; 1. Redundancy (The Survival Strategy) Redundancy is simply the duplication of components to increase reliability. Ideally, it\u0026rsquo;s about Availability. It does not imply that the components share state or data.\nExample: You have two web servers running the same code. If Server A dies, Server B takes over. Since the code is static, you don\u0026rsquo;t need to \u0026ldquo;sync\u0026rdquo; anything in real-time. Types of Redundancy Active Redundancy: All nodes are active and sharing the load (e.g., behind a Load Balancer). Passive Redundancy: One node is active, the others are on standby (Cold Standby). %%{init: {\u0026#39;theme\u0026#39;: \u0026#39;dark\u0026#39;}}%% graph TD subgraph Active_Redundancy [Active Redundancy] LB[Load Balancer] --\u0026gt; S1[Server A] LB --\u0026gt; S2[Server B] S1 -.-\u0026gt;|Both Active| S2 end subgraph Passive_Redundancy [Passive Redundancy] LB2[Load Balancer] --\u0026gt; M1[Primary] M1 -.-\u0026gt;|Failover| M2[Standby] style M2 stroke-dasharray: 5 5 end 2. Replication (The Data Strategy) Replication is Redundancy + Synchronization. It is used for Data (Databases, Caches) where state changes over time. You can\u0026rsquo;t just have a specific \u0026ldquo;backup\u0026rdquo; DB; that backup must have the latest data.\nExample: You have a Primary Database and a Replica. When you write User.name = \u0026quot;Alice\u0026quot; to the Primary, that change must be Replicated to the Replica. Types of Replication Active Replication (Multi-Master): You can write to any node, and they sync with each other. Complex conflict resolution needed. Passive Replication (Master-Slave): You write only to the Master. The Master syncs to Slaves. Slaves are Read-Only (usually). %%{init: {\u0026#39;theme\u0026#39;: \u0026#39;dark\u0026#39;}}%% graph TD Client --\u0026gt;|Write| Master[(Master DB)] Master --\u0026gt;|Sync Data| Slave1[(Replica 1)] Master --\u0026gt;|Sync Data| Slave2[(Replica 2)] Client --\u0026gt;|Read| Slave1 Client --\u0026gt;|Read| Slave2 Comparison Cheat Sheet Feature Redundancy Replication Goal Availability (Survive failure) Consistency (Keep data in sync) Sync Needed? No (Usually stateless) Yes (Critical) Use Case Web Servers, Power Supplies, Network Cables Databases, Caches, File Storage Complexity Low High (Handling lag, conflicts) Conclusion Use Redundancy to keep your application running (Stateless). Use Replication to keep your data safe and consistent (Stateful). ","date":"2026-02-08T00:00:00Z","permalink":"/blogs/system-design/redundancy-vs-replication/","title":"Redundancy vs Replication: Spot the Difference"},{"content":"When your app goes viral, your server will crash. To fix this, you have two choices:\nMake your existing server stronger (Vertical Scaling). Add more servers to share the load (Horizontal Scaling). What is Scalability? Scalability is a system\u0026rsquo;s ability to handle an increased load without degraded performance.\n1. Vertical Scaling (Scale Up) You upgrade the existing machine. You add more RAM, a faster CPU, or a bigger SSD.\nAnalogy: Your car is too slow, so you replace the engine with a Ferrari engine. It\u0026rsquo;s still one car, but much faster. Pros: Simple. No code changes required. Lower management overhead. Cons: Expensive. Has a hard limit (you can\u0026rsquo;t upgrade forever). Single Point of Failure. %%{init: {\u0026#39;theme\u0026#39;: \u0026#39;dark\u0026#39;}}%% graph TD User --\u0026gt; Server[Small Server] Server --\u0026gt;|Upgrade| BigServer[BIG Server] style BigServer fill:#ff9900,stroke:#333,stroke-width:4px 2. Horizontal Scaling (Scale Out) You add more machines to your pool of resources.\nAnalogy: Your car can\u0026rsquo;t carry enough people, so you buy a second car (and a third, and a fourth). Pros: Infinite scaling. Cheaper hardware. High Availability. Cons: Complex. Requires Load Balancers. Data consistency challenges. %%{init: {\u0026#39;theme\u0026#39;: \u0026#39;dark\u0026#39;}}%% graph TD User --\u0026gt; LB[Load Balancer] LB --\u0026gt; S1[Server 1] LB --\u0026gt; S2[Server 2] LB --\u0026gt; S3[Server 3] style LB fill:#f9f,stroke:#333 Comparisons Feature Vertical Scaling (Scale Up) Horizontal Scaling (Scale Out) Complexity Low (Easy) High (Hard) Cost High (Premium Hardware) Low (Commodity Hardware) Limit Hard Hardware Limit Theoretically Unlimited Failover Single Point of Failure Resilient (Redundancy) Downtime Required for upgrades Zero downtime upgrades Conclusion Start with Vertical Scaling for simplicity (Monoliths). Switch to Horizontal Scaling when you need reliability and massive scale (Distributed Systems). ","date":"2026-02-08T00:00:00Z","permalink":"/blogs/system-design/scalability/","title":"Scalability: Vertical vs Horizontal Scaling"},{"content":"In distributed systems, there is no free lunch. The CAP Theorem (Brewer\u0026rsquo;s Theorem) states that a distributed data store can effectively provide only two of the following three guarantees:\nConsistency (C) Availability (A) Partition Tolerance (P) The Three Pillars 1. Consistency (C) Every read receives the most recent write or an error.\nAnalogy: If you update your status on Facebook on your phone, your friend on their laptop should see the new status immediately. If they see the old one, the system is Inconsistent. 2. Availability (A) Every request receives a (non-error) response, without the guarantee that it contains the most recent write.\nAnalogy: Even if the network is flaky, Amazon should allow you to add items to your cart. It’s better to sell the item and sync later than to show a \u0026ldquo;System Offline\u0026rdquo; error. 3. Partition Tolerance (P) The system continues to operate despite an arbitrary number of messages being dropped or delayed by the network between nodes.\nAnalogy: If the cable connecting the US and Europe servers is cut, the system should still work. The Reality: P is Not Optional In a Distributed System, network failures (Partitions) are inevitable. You cannot choose \u0026ldquo;CA\u0026rdquo; (Consistency + Availability) because that implies your network will never fail, which is impossible.\nTherefore, the real choice is between CP and AP.\nCP (Consistency + Partition Tolerance) Philosophy: \u0026ldquo;Better to return Error than Wrong Data.\u0026rdquo; Behavior: When a partition occurs, the system stops accepting writes to preserve consistency. Use Case: Banking, ATM. You cannot allow two peole to withdraw the same $100 if the ATMs lose connection. The ATM simply says \u0026ldquo;Out of Service\u0026rdquo; (Sacrificing Availability). AP (Availability + Partition Tolerance) Philosophy: \u0026ldquo;The Show Must Go On.\u0026rdquo; Behavior: When a partition occurs, nodes continue to accept writes and serve stale data. They sync up when the partition heals. Use Case: Social Media, Shopping Carts. If Instagram cannot sync your latest photo globally, it will still show your profile. It stays Available, even if Inconsistent. graph TD subgraph CP [CP System - Banking] A[ATM 1] -. X .- B[Bank Database] linkStyle 0 stroke-width:2px,fill:none,stroke:red; User1[User] --\u0026gt; A A -- \u0026#34;Service Unavailable\u0026#34; --\u0026gt; User1 end subgraph AP [AP System - Social Media] C[Server 1] -. X .- D[Server 2] linkStyle 3 stroke-width:2px,fill:none,stroke:red; User2[User] --\u0026gt; C C -- \u0026#34;Here is older data\u0026#34; --\u0026gt; User2 %% Note: C Accepts Writes locally end Summary Table Choice Trade-off Example CA No Partition Tolerance RDBMS (MySQL) (Single Node) CP Sacrifice Availability MongoDB, HMAC (Banking) AP Sacrifice Consistency Cassandra, DynamoDB (Social Media) Conclusion The CAP Theorem forces you to decide what matters more to your business.\nIf losing a transaction means legal trouble (Money), choose CP. If downtime means losing customers (Retail/Social), choose AP. There is no \u0026ldquo;best\u0026rdquo; architecture, only the right one for your specific problem.\n","date":"2026-02-08T00:00:00Z","permalink":"/blogs/system-design/cap-theorem/","title":"The CAP Theorem: You Can't Have It All"},{"content":"Before databases (DBMS) ruled the world, we had File-Based Storage Systems. Essentially, data was stored in folders and text files. While simple, it falls apart quickly at scale.\nWhat is a File-Based System? Imagine storing college student records in a folder structure:\nFolder: Computer Science -\u0026gt; File: Ram.txt Folder: Electrical Engineering -\u0026gt; File: Shyam.txt The 4 Major Challenges 1. Data Redundancy The same data is stored in multiple places.\nExample: \u0026ldquo;Ram\u0026rdquo; takes both CS and Math classes. His address is stored in CS/Ram.txt AND Math/Ram.txt. Result: Wasted storage. 2. Data Inconsistency Since data is duplicated, updates might fail on one copy.\nScenario: Ram moves to a new house. You update CS/Ram.txt but forget Math/Ram.txt. Result: The system has two different addresses for the same person. Which one is true? %%{init: {\u0026#39;theme\u0026#39;: \u0026#39;dark\u0026#39;}}%% graph TD User[\u0026#34;User\u0026#34;] --\u0026gt; Update[\u0026#34;Update Address\u0026#34;] Update --\u0026gt; File1[\u0026#34;CS/Ram.txt (Updated)\u0026#34;] Update -.-\u0026gt;|Forgot to Update| File2[\u0026#34;Math/Ram.txt (Old Address)\u0026#34;] style File1 fill:#99ff99,stroke:#333 style File2 fill:#ff9999,stroke:#333 3. Security Issues It\u0026rsquo;s hard to control granular access.\nScenario: You want a professor to see Ram\u0026rsquo;s grades but not his home address. Problem: In a file system, if you give read access to the file, they see everything. You can\u0026rsquo;t easily hide specific \u0026ldquo;columns\u0026rdquo;. 4. Slow Data Access No indexing.\nScenario: \u0026ldquo;Find all students with GPA \u0026gt; 3.5\u0026rdquo;. Problem: You have to open every single file and check. This is incredibly slow compared to a SQL query. Conclusion File-based systems are fine for simple, personal data. But for any application with multiple users and relationships, you need a Relational Database Management System (RDBMS) to handle Consistency, Security, and Speed.\n","date":"2026-02-08T00:00:00Z","permalink":"/blogs/system-design/file-based-storage/","title":"The File-Based Storage Nightmare"},{"content":"In our previous discussions, we covered Monoliths, Distributed Systems, and Latency.\nToday, we decode Throughput (often measured in bits per second, bps)—the measure of how much work your system can actually handle. While Latency is about \u0026ldquo;speed,\u0026rdquo; Throughput is about \u0026ldquo;capacity.\u0026rdquo;\nWhat is Throughput? Throughput is the amount of work (requests, data, transactions) a system can process in a given unit of time.\nLatency: How long does it take to move one car from A to B? Throughput: How many cars per hour can move from A to B? Ideally, you want High Throughput and Low Latency. However, improving one can sometimes hurt the other.\nThroughput: Monolithic vs. Distributed Feature Monolithic System Distributed System Resources Limited to one machine (CPU/RAM cap) Virtually unlimited (Add more machines) Scaling Vertical (Expensive, Hard limit) Horizontal (Add nodes endlessly) Throughput Low/Capped High/Uncapped Bottleneck The single server itself Network bandwidth or Database Why Distributed Systems Win on Throughput In a Monolith, your throughput is hard-capped by the physical limits of the server. If your server can handle 1,000 requests/sec, that\u0026rsquo;s it.\nIn a Distributed System, you can use Horizontal Scaling. If one server handles 1,000 req/sec, ten servers handle 10,000 req/sec.\ngraph TD LB[Load Balancer] --\u0026gt; S1[Server 1] LB --\u0026gt; S2[Server 2] LB --\u0026gt; S3[Server 3] S1 --\u0026gt; DB[(Database)] S2 --\u0026gt; DB S3 --\u0026gt; DB style LB fill:#f9f,stroke:#333 By adding Servers (S1, S2, S3\u0026hellip;), you increase the total throughput of the system linearly.\nWhat Kills Throughput? Even in distributed systems, throughput isn\u0026rsquo;t infinite. Three main factors drag it down:\nLatency: If each request takes longer to process (high latency), the server is busy for longer, reducing the number of requests it can handle per second. Protocol Overhead: Every network call involves handshakes (TCP/TLS) and headers. This \u0026ldquo;administrative\u0026rdquo; data consumes bandwidth that could be used for actual payload. Congestion: When too many requests arrive at once, queues fill up. Packets get dropped, causing retries, which further clogs the system. Strategies to Maximize Throughput To get the most out of your distributed system, you need to optimize the flow of data.\n1. Load Balancing A Load Balancer sits in front of your servers and distributes traffic evenly (e.g., Round Robin). This ensures no single server is overwhelmed while others sit idle.\n2. Caching \u0026amp; CDNs Serving data from a cache or CDN avoids hitting the backend servers entirely.\nReal-Life Example: Netflix. By serving video chunks from a local ISP server (CDN), they offload terabytes of data from their main servers, massively increasing global throughput. 3. Asynchronous Processing Instead of blocking a connection while waiting for a task to finish, use message queues (like Kafka). The system accepts the request immediately (improving throughput) and processes it in the background.\nReal-Life Example: Ticket Booking vs. Video Streaming Ticket Booking (IRCTC/Ticketmaster): Low Throughput, High Consistency requirement. The bottleneck is the database lock (selling the same seat twice). Video Streaming (YouTube): High Throughput requirement. The system needs to push gigabytes of data per second to millions of users. It relies heavily on CDNs and distributed storage to achieve this massive throughput. Conclusion Throughput is the capacity of your system. While Monoliths are limited by single-machine physics, Distributed Systems unlock massive throughput via Horizontal Scaling. By managing Congestion and using Load Balancers, you can build systems that handle millions of users effortlessly.\n","date":"2026-02-08T00:00:00Z","permalink":"/blogs/system-design/throughput-in-distributed-systems/","title":"Throughput in Distributed Systems: The Impact of Scale"},{"content":"In a Monolithic system, \u0026ldquo;Time\u0026rdquo; is easy. You ask the OS for Date.now(), and that is the absolute truth. In a Distributed System, \u0026ldquo;Time\u0026rdquo; is a nightmare.\nCommon questions like \u0026ldquo;Which event happened first?\u0026rdquo; become remarkably hard to answer.\nThe Problem with Physical Clocks Distributed systems span different machines, often in different time zones (e.g., US, India, Australia). Each machine has its own Physical Clock. Even with NTP (Network Time Protocol), these clocks drift. One server might think it\u0026rsquo;s 10:00:00.001 while another thinks it\u0026rsquo;s 10:00:00.005.\nThis Clock Skew means you cannot rely on timestamps to order events.\nScenario: User A books a ticket at 10:00:01 (Server US). User B books the same ticket at 10:00:02 (Server India). If the India server\u0026rsquo;s clock is 5 seconds slow, it might record the time as 09:59:57, making it look like User B booked it first! Ideally: Partial Ordering We don\u0026rsquo;t need to know the exact time. We just need to know the Order of Events. This is called the Happened-Before Relationship.\nThe Solution: Lamport Logical Clocks Leslie Lamport introduced a simple algorithm to solve this using Logical Counters instead of real time.\nThe Algorithm Every process maintains a counter C, initially 0.\nInternal Event: Before executing an event, increment C = C + 1. Send Message: Increment C = C + 1 and send C along with the message. Receive Message: When receiving a message with counter C_msg, update your local counter: C = max(C, C_msg) + 1. Visualizing the Flow Let\u0026rsquo;s look at three processes (P1, P2, P3) interacting. Notice how P2\u0026rsquo;s clock jumps forward when it receives a message from P3.\n%%{init: {\u0026#39;theme\u0026#39;: \u0026#39;dark\u0026#39;}}%% sequenceDiagram participant P1 participant P2 participant P3 Note over P1,P3: Initial State: All Clocks = 0 P1-\u0026gt;\u0026gt;P1: Event A (C=1) P2-\u0026gt;\u0026gt;P2: Event B (C=1) P1-\u0026gt;\u0026gt;P2: Message (C=2) Note right of P2: P2 receives C=2.\u0026lt;br/\u0026gt;max(1, 2) + 1 = 3 P2-\u0026gt;\u0026gt;P2: Event C (C=3) P3-\u0026gt;\u0026gt;P3: Event D (C=1) P3-\u0026gt;\u0026gt;P2: Message (C=2) Note right of P2: P2 receives C=2.\u0026lt;br/\u0026gt;max(3, 2) + 1 = 4 P2-\u0026gt;\u0026gt;P2: Event E (C=4) P2-\u0026gt;\u0026gt;P1: Message (C=5) Note left of P1: P1 receives C=5.\u0026lt;br/\u0026gt;max(1, 5) + 1 = 6 P1-\u0026gt;\u0026gt;P1: Event F (C=6) What This Tells Us If event A caused event B, then Clock(A) \u0026lt; Clock(B). This gives us a Partial Ordering of events. It does not give us real time (duration), but it ensures causality is respected. Conclusion In Distributed Systems, Logical Time \u0026gt; Physical Time. Tools like Lamport Clocks and Vector Clocks (an immense improvement on Lamport) are the bedrock of systems like Amazon Dynamo, Cassandra, and Google Spanner. They allow us to agree on the sequence of events without needing atomic clocks in every server.\n","date":"2026-02-08T00:00:00Z","permalink":"/blogs/system-design/time-in-distributed-systems/","title":"Time in Distributed Systems: Why Clocks Lie"},{"content":"Speed is a feature. In system design, Latency is the villain we are constantly fighting.\nIn the previous post, we discussed Distributed Systems, which provide scalability at the cost of complexity. One of the biggest costs? Latency.\nIn this post, we\u0026rsquo;ll dissect what latency actually is, why modern distributed architectures often make it worse, and the three main weapons we have to fight it.\nWhat is Latency? Latency is the time it takes for a data packet to travel from one point to another. In the context of a web application, it\u0026rsquo;s the round-trip time from the user\u0026rsquo;s action to the application\u0026rsquo;s response.\nThe Formula: T1 + T2 + T3 We can break down latency into three distinct phases:\nT1 (Network Delay - Request): Time for the request to travel from User → Server. T2 (Processing Delay - Computer): Time the server takes to think and process the request. T3 (Network Delay - Response): Time for the response to travel from Server → User. [ \\text{Total Latency} = T1 + T2 + T3 ]\nsequenceDiagram participant User participant Server Note over User, Server: T1 (Network Delay) User-\u0026gt;\u0026gt;Server: Request Note over Server: T2 (Processing) Note over Server, User: T3 (Network Delay) Server--\u0026gt;\u0026gt;User: Response Architecture Impact: Monolith vs. Distributed You might think that upgrading to a \u0026ldquo;modern\u0026rdquo; Distributed System (microservices) would make your app faster. Often, it does the opposite.\nMonolithic Architecture (Low Latency) In a monolith, function calls are strictly in-memory. When the OrderService needs to check the InventoryService, it\u0026rsquo;s just a function call.\nLatency Profile: Extremely low network overhead. Mostly T2 (Processing). Distributed Architecture (High Latency) In a distributed system, that same check is now an HTTP request over the network.\nLatency Profile: Every internal communication adds a new T1 + T3 round trip. Architecture Processing Delay (T2) Network Delay (T1 + T3) Overall Latency Monolithic Standard Minimal (External only) Low Distributed Standard High (Internal + External) High How to Reduce Latency Since we can\u0026rsquo;t change the speed of Network (T1/T3) or make CPUs infinitely fast (T2), we use architectural patterns.\n1. Caching (Reducing T2) If your application takes 500ms to calculate a report (T2), doing it every time is wasteful. Caching stores the result so the next request takes 5ms.\nReal-Life Example: Twitter/X. When you load your timeline, Twitter doesn\u0026rsquo;t query the database for every tweet. It pulls a pre-computed list from a Redis cache, making the load time instant. graph LR User --\u0026gt; Cache Cache -- Hit --\u0026gt; User Cache -- Miss --\u0026gt; Server Server --\u0026gt; Database 2. Content Delivery Network - CDN (Reducing T1 + T3) If your server is in New York and your user is in Tokyo, T1 and T3 will be high because of physical distance. A CDN stores copies of your static files (images, CSS, JS) on servers all over the world.\nReal-Life Example: Instagram. The photos you see aren\u0026rsquo;t coming from a main server in the US; they are being served from a CDN edge location in your own city. 3. Hardware Scaling (Reducing T2) Sometimes, the simplest solution is brute force. upgrading the CPU, RAM, or using faster SSDs can significantly reduce processing time.\nBenefit: Easy to implement. Downside: Expensive and has limits (Vertical Scaling limit). Conclusion Latency is the silent killer of user experience. While distributed systems offer scalability, they introduce network latency that must be managed. By understanding the T1+T2+T3 formula and aggressively using Caching and CDNs, you can build systems that feel instant to your users.\n","date":"2026-02-08T00:00:00Z","permalink":"/blogs/system-design/latency-in-web-applications/","title":"Understanding Latency in Web Applications: Causes and Cures"},{"content":"In the previous post, we explored Monolithic Architecture, where everything lives in one big box. While simple, monoliths struggle to scale beyond a certain point.\nEnter Distributed Systems—the architecture that powers the modern internet. From Google Search to Netflix, distributed systems are what allow applications to handle millions of concurrent users without crashing.\nWhat is a Distributed System? A Distributed System is a collection of independent computers that appear to its users as a single coherent system.\nInstead of one massive server doing everything, you have multiple machines (nodes) communicating over a network to achieve a common goal.\nMonolithic vs. Distributed Feature Monolithic System Distributed System Deployment Single server/location Multiple machines across networks Scaling Vertical (Add more RAM/CPU to one machine) Horizontal (Add more machines) Failure Single Point of Failure (If server dies, app dies) Fault Tolerant (If one node dies, others take over) Complexity Low High Key Advantages 1. Fault Tolerance (No Single Point of Failure) In a monolith, if the server crashes, your business stops. In a distributed system, data and services are replicated across multiple machines.\nReal-Life Example: If one Google data center goes offline due to a power outage, your search query is simply routed to another data center. You don\u0026rsquo;t even notice the failure. 2. Horizontal Scalability This is the superpower of distributed systems.\nVertical Scaling (Monolith): Upgrading a server is like buying a bigger truck. Eventually, you can\u0026rsquo;t buy a bigger truck. Horizontal Scaling (Distributed): Simply buy more trucks. You can add cheap, commodity hardware to the cluster to handle increased load endlessly. 3. Low Latency (Geo-Distribution) You can place servers closer to your users.\nReal-Life Example: Netflix. When you stream a movie, you aren\u0026rsquo;t downloading it from Netflix HQ in California. You are streaming it from an Open Connect appliance (CDN) located at your local ISP\u0026rsquo;s data center, ensuring instant buffering. Challenges: The Price of Power Distributed systems are powerful, but they are hard to build and manage.\n1. Data Consistency If you have data on five different machines, how do you ensure they all agree?\nScenario: User A updates their profile picture on Node 1. User B views the profile on Node 2. If replication is slow, User B sees the old photo. This is the classic Consistency vs. Availability trade-off (CAP Theorem). 2. Network Unreliability Calls within a monolith are instant function calls. Calls in a distributed system go over a network. Networks are unreliable—packets get lost, latency spikes, and connections drop.\n3. Operational Complexity Managing one server is easy. Managing a fleet of 1,000 servers requires sophisticated tools for:\nOrchestration (Kubernetes) Monitoring (Prometheus, Grafana) Logging (ELK Stack) Real-Life Example: Google Search Google Search is the ultimate distributed system.\nCrawling: Thousands of machines crawl the web in parallel. Indexing: The index is too large for one computer, so it\u0026rsquo;s sharded across thousands of machines. Searching: When you type a query, it\u0026rsquo;s sent to hundreds of machines that search their specific shard of the index in parallel. The results are then aggregated and returned to you in milliseconds. Conclusion Distributed Systems are the answer to \u0026ldquo;How do we scale big?\u0026rdquo; They offer resilience and infinite scaling but introduce significant complexity in data management and operations. For most startups, a Monolith is the right start. For a global enterprise, a Distributed System is a necessity.\n","date":"2026-02-07T00:00:00Z","permalink":"/blogs/system-design/distributed-systems/","title":"Distributed Systems: Definition, Advantages, and Challenges"},{"content":"System Design is one of the most critical skills for software engineers, especially as they progress to senior roles. It’s the process of defining the architecture, modules, interfaces, and data for a system to satisfy specified requirements.\nIn simple terms, if coding is about how to write a function, System Design is about where that function lives, how it talks to other parts of the application, and how the entire system scales to handle millions of users.\nWhat is System Design? System Design is the process of defining the elements of a system—such as architecture, modules, and components—and their interfaces and data for a system to satisfy specified requirements. It is the bridge between a problem statement and the final code.\nThe main objective is to define and organize the elements of an application to ensure that all parts work cohesively. A well-designed system is:\nScalable: Can handle growth in users and data. Reliable: Functions correctly even when components fail. Maintainable: Easy to understand and modify. Types of System Design In the context of software engineering, System Design is generally categorized into two main types: High-Level Design (HLD) and Low-Level Design (LLD).\n1. High-Level Design (HLD) \u0026ldquo;The Big Picture\u0026rdquo;\nHLD focuses on the overall system architecture. It identifies the major components of the system and how they interact with each other. It answers questions like:\n\u0026ldquo;Microservices or Monolith?\u0026rdquo; \u0026ldquo;SQL or NoSQL?\u0026rdquo; \u0026ldquo;How do we handle caching?\u0026rdquo; Real-Life Example: Designing Uber (HLD) When designing the HLD for Uber, you wouldn\u0026rsquo;t worry about the specific class for a Driver. Instead, you\u0026rsquo;d define:\nLoad Balancers to distribute traffic. Microservices for User Management, Ride Matching, and Payments. Database choices (e.g., PostgreSQL for transactions, Cassandra for location history). Communication Protocols (e.g., WebSockets for real-time location updates). 2. Low-Level Design (LLD) \u0026ldquo;The Detailed View\u0026rdquo;\nLLD dives into the implementation details of the components defined in the HLD. It focuses on the internal logic of modules, data structures, algorithms, and class diagrams.\nReal-Life Example: Designing the RideMatching Service (LLD) Continuing with the Uber example, LLD would define:\nClass Diagrams: A Driver class, a Rider class, and a Trip class. Interfaces: Public methods like findNearestDriver(location). Database Schema: The exact columns in the Trips table. Algorithms: The specific logic used to match a rider with a driver (e.g., using a QuadTree for geospatial search). The System Design Process Designing a system is not a linear process, but it generally follows a structured flow:\nStep 1: Requirements Gathering Before drawing any diagrams, you must understand what you are building.\nFunctional Requirements: What should the system do? (e.g., \u0026ldquo;Users can post tweets\u0026rdquo;). Non-Functional Requirements: How should the system behave? (e.g., \u0026ldquo;The system must be highly available\u0026rdquo;). Step 2: Define Architecture (HLD) Decide on the high-level structure.\nMonolith vs. Microservices: Choose based on team size and complexity. Data Flow: How does data move from the user to the database? Step 3: Identify Modules Break the system down into smaller, manageable chunks.\nFor an E-commerce site: Inventory Service, User Service, Cart Service, Order Service. Step 4: Define Interaction Points Specify how these modules will communicate.\nSynchronous: REST API or gRPC. Asynchronous: Message Queues (Kafka, RabbitMQ). Step 5: Data Modeling and Logic (LLD) Define the database schema and business logic.\nSchema Design: Tables, relationships, and indexes. API Design: Define the endpoints (e.g., POST /api/v1/checkout). Summary Feature High-Level Design (HLD) Low-Level Design (LLD) Focus Overall Architecture Component Implementation Audience Architects, Stakeholders Developers Deliverables System Diagrams, Tech Stack Class Diagrams, DB Schema Example \u0026ldquo;Use Redis for caching\u0026rdquo; \u0026ldquo;Use LRUCache class with HashMap\u0026rdquo; Conclusion System Design is finding the optimal solution to a problem under constraints. Whether you are building a small startup app or a global platform like Netflix, understanding the distinction between HLD and LLD and following a structured design process is key to success.\nStay tuned for more deep dives into specific System Design components like Load Balancing, Caching strategies, and Database Sharding!\n","date":"2026-02-07T00:00:00Z","permalink":"/blogs/system-design/introduction-to-system-design/","title":"Introduction to System Design: Concepts, Process, and Types"},{"content":"In the world of software architecture, \u0026ldquo;Monolithic\u0026rdquo; often gets a bad rap, associated with \u0026ldquo;legacy\u0026rdquo; or \u0026ldquo;old-school\u0026rdquo; code. But the truth is, Monolithic Architecture is the foundation upon which many of today\u0026rsquo;s tech giants were built. It remains the most practical choice for many new projects.\nThis post breaks down what Monolithic Architecture really is, its core components, and why you might—or might not—want to use it.\nWhat is Monolithic Architecture? Monolithic Architecture is a unified model for designing a software application. In this approach, the application is built as a single, indivisible unit.\nThink of it like a massive Lego castle where every brick is glued together. You can\u0026rsquo;t just take off a tower and replace it without potentially affecting the stability of the entire structure.\nThe \u0026ldquo;Unified\u0026rdquo; Concept In a typical web application, you have three main layers:\nFrontend: The User Interface (UI). Backend: The Business Logic. Data Layer: The Database interface. In a Monolith, all these layers are managed within a single codebase and deployed as a single executable. Even if the database itself runs on a separate server (like a managed PostgreSQL instance), the code that interacts with it is tightly bundled with the business logic and API endpoints.\nCharacteristics of a Monolith Single Codebase: All features (User Auth, Payments, Inventory) live in one Git repository. Unified Deployment: To update one line of code, you must redeploy the entire application. Shared Memory: Components can call each other directly (function calls) rather than over a network (API calls), making communication extremely fast. Advantages: Why Start with a Monolith? Simplicity: It\u0026rsquo;s much easier to develop, test, and deploy one application than twenty microservices. \u0026ldquo;git push heroku master\u0026rdquo; is the epitome of Monolithic simplicity. Lower Latency: Communication between modules happens in-memory. There\u0026rsquo;s no network overhead of serializing JSON and making HTTP requests between services. Easier Debugging: You can trace a request from start to finish in a single IDE window. No need for complex distributed tracing tools. Consistency: It\u0026rsquo;s easier to enforce code standards and transactional integrity (ACID) when everything shares the same database. Disadvantages: The Growing Pains Tight Coupling: A bug in the \u0026ldquo;Recommendations\u0026rdquo; module could crash the entire application, including \u0026ldquo;Checkout\u0026rdquo;. Scalability Issues: You can\u0026rsquo;t scale just the \u0026ldquo;Video Processing\u0026rdquo; part of your app. You have to scale the entire monolith, which wastes resources. \u0026ldquo;Dependency Hell\u0026rdquo;: Updating a library for one module might break another module that relies on an older version. Slow Build \u0026amp; Deploy: As the codebase grows to millions of lines, compiling and deploying can take upwards of 30-40 minutes. Real-Life Examples 1. The Startup MVP (Most Common) Almost every successful startup—Airbnb, Uber, Instagram—started as a Monolith. Why? Because when you are validating a product, speed of iteration is everything. The overhead of managing microservices slows you down. Example: A simple E-commerce store built with Ruby on Rails or Django. The storefront, admin panel, and payment processing are all in one app.\n2. Stack Overflow (The Majestic Monolith) Stack Overflow, one of the highest-traffic sites on the internet, runs on a monolithic architecture (.NET). They prove that with proper optimization (caching, efficient DB queries), a monolith can scale to handle billions of requests. They didn\u0026rsquo;t need microservices to succeed; they needed good engineering.\n3. Shopify (Modular Monolith) Shopify is another giant that famously stuck with a monolith for a long time. They eventually evolved into a Modular Monolith—where the code is still in one repo and deploys together, but the internal boundaries are strictly enforced to prevent \u0026ldquo;spaghetti code.\u0026rdquo;\nConclusion: Monolith or Not? Choose a Monolith if:\nYou are a startup or building an MVP. Your team is small (e.g., \u0026lt; 10 developers). You want to focus on business features, not infrastructure complexity. Consider Distributed/Microservices if:\nYou have large, independent teams that need to deploy separately. Specific parts of your app need independent scaling (e.g., video encoding). The monolith has become too large to build and test efficiently. Monolithic Architecture is not \u0026ldquo;outdated\u0026rdquo;—it\u0026rsquo;s a valid, powerful design choice. The key is knowing when it fits your needs and when it\u0026rsquo;s time to break it apart.\n","date":"2026-02-07T00:00:00Z","permalink":"/blogs/system-design/monolithic-architecture/","title":"Understanding Monolithic Architecture: The 'All-in-One' Approach"},{"content":"It’s the end of an era for the iOS ecosystem. After 15 years as the de facto dependency manager for iOS, CocoaPods is retiring.\nMark your calendar: December 2, 2026. That is the day the CocoaPods trunk goes permanently read-only. While existing pods will remain accessible, no new pods or updates can be published. It\u0026rsquo;s time to move on.\nThe Legacy of CocoaPods Before we rush to migrate, it\u0026rsquo;s worth acknowledging what CocoaPods achieved. It brought order to the chaos of manual dependency management (remember dragging .xcodeproj files and linking binaries manually?). It democratized open source in the Apple ecosystem. But as Swift has matured, the tooling has evolved.\nWhy Move to Swift Package Manager (SPM)? Apple\u0026rsquo;s official answer to dependency management, Swift Package Manager, has grown up.\nNative Integration: It\u0026rsquo;s built directly into Xcode. No context switching to terminals or managing a separate workspace. No Ruby: Say goodbye to rbenv, Gemfile, and Ruby version conflicts that break your CI/CD. Performance: SPM leverages Swift\u0026rsquo;s build system for faster dependency resolution and indexing. Migration Strategy: The \u0026ldquo;Long Way\u0026rdquo; Home Migrating a large project isn\u0026rsquo;t just about deleting a file. It requires a strategy.\nStep 1: Audit Your Dependencies First, list every pod you currently use. Check their repositories to see if they support SPM (most popular ones like Alamofire, Kingfisher, and Lottie already do).\nflowchart TD Start[Start Migration] --\u0026gt; Audit{Audit Dependencies} Audit --\u0026gt;|All support SPM?| Simple[Switch to SPM] Audit --\u0026gt;|Some missing?| CheckAlt[Check Alternatives/Forks] CheckAlt --\u0026gt;|Found?| Simple CheckAlt --\u0026gt;|No support?| Hybrid[Keep minimal Podfile] Simple --\u0026gt; RemovePod[Remove Podfile] Hybrid --\u0026gt; Reduce[Migrate what you can] Step 2: Clean Up CocoaPods Before adding new packages, you need to strip out the old integration.\nDeintegrate: Run the following command in your project directory: 1pod deintegrate Clean Files: Delete Podfile, Podfile.lock, and the .xcworkspace. From now on, you\u0026rsquo;ll open the .xcodeproj directly. Step 3: Adding Packages in Xcode This is where the magic happens.\nAction Required:\nOpen Xcode. Go to File \u0026gt; Add Package Dependencies\u0026hellip; Paste the URL of the package repository (e.g., https://github.com/Alamofire/Alamofire). Choose your version rules (typically \u0026ldquo;Up to Next Major\u0026rdquo;). (Insert Screenshot here: Show the Xcode \u0026ldquo;Add Package Dependency\u0026rdquo; dialog with a search result visible)\nStep 4: Handling Resources and Objective-C SPM handles resources (images, json, xibs) differently.\nBundles: If you were accessing resources via a specific bundle ID in CocoaPods, you might need to switch to using Bundle.module (available automatically in SPM packages). Obj-C Flags: If you have legacy Objective-C dependencies, ensure you check the \u0026ldquo;Build Settings\u0026rdquo; for Linker Flags. SPM usually handles this, but sometimes you may need to add -ObjC manually if a comprehensive static library is involved. Common Pitfalls Dynamic vs Static: CocoaPods defaults to static libraries (unless use_frameworks! is set). SPM tries to interpret the package manifest. If you get duplicate symbol errors, check if you are linking a static library in multiple targets. CI/CD: Update your pipeline (.github/workflows or Fastlane). You no longer need pod install. Instead, add a step to resolve package versions if you cache them: 1xcodebuild -resolvePackageDependencies Conclusion The deadline is 2026, but the best time to migrate is now. By moving to SPM, you simplify your project capability and align with the future of Apple development.\nStart now. Go slow. And yeah — thanks for everything, CocoaPods!\n","date":"2026-01-31T00:00:00Z","image":"/blogs/cocoapods-retiring/cocoapods-migration.png","permalink":"/blogs/cocoapods-retiring/","title":"CocoaPods is Retiring: A Complete Migration Guide to SPM"},{"content":"Linked Lists are one of the most fundamental data structures in computer science. While Swift\u0026rsquo;s standard library provides powerful Array and Set types, understanding Linked Lists is crucial for coding interviews and understanding memory management.\nWhat is a Linked List? A Linked List is a linear data structure where elements are not stored in contiguous memory locations. Instead, each element (called a Node) points to the next one.\nLinked List vs Array Feature Array Linked List Memory Contiguous block Scattered (connected by pointers) Insertion/Deletion Expensive (Shift elements) Cheap (Update pointers) Access O(1) Random Access O(n) Sequential Access Anatomy of a Node Each node contains two things:\nValue: The data it holds. Next: A reference to the next node. graph LR Head[HEAD] --\u0026gt; Node1 subgraph Node1 [Node 1] Val1[Value: 10] Next1[Next] end Next1 --\u0026gt; Node2 subgraph Node2 [Node 2] Val2[Value: 20] Next2[Next] end Next2 --\u0026gt; Null[Nil] Implementation in Swift Let\u0026rsquo;s build a Generic Node class.\n1class Node\u0026lt;T\u0026gt; { 2 var value: T 3 var next: Node? 4 5 init(value: T) { 6 self.value = value 7 } 8} Now, the LinkedList class to manage the nodes.\n1struct LinkedList\u0026lt;T\u0026gt; { 2 var head: Node\u0026lt;T\u0026gt;? 3 var tail: Node\u0026lt;T\u0026gt;? 4 5 var isEmpty: Bool { 6 return head == nil 7 } 8 9 // 1. Append (Add to end) - O(1) thanks to tail pointer 10 mutating func append(_ value: T) { 11 let newNode = Node(value: value) 12 if let tailNode = tail { 13 tailNode.next = newNode 14 } else { 15 head = newNode 16 } 17 tail = newNode 18 } 19 20 // 2. Prepend (Add to start) - O(1) 21 mutating func prepend(_ value: T) { 22 let newNode = Node(value: value) 23 newNode.next = head 24 head = newNode 25 if tail == nil { 26 tail = head 27 } 28 } 29} Advanced Operations Inserting at a Specific Index To insert into the middle, we must traverse the list to find the node before the target index.\ngraph LR A[A] --\u0026gt; B[B] B --\u0026gt; C[C] style B fill:#f9f,stroke:#333,stroke-width:2px style New fill:#bbf,stroke:#333,stroke-width:2px New[New Node] -.-\u0026gt; C B -.-\u0026gt; New 1 mutating func insert(_ value: T, after node: Node\u0026lt;T\u0026gt;) { 2 let newNode = Node(value: value) 3 newNode.next = node.next 4 node.next = newNode 5 if newNode.next == nil { 6 tail = newNode 7 } 8 } Removing a Value Removing requires updating the next pointer of the previous node to skip the current one.\n1 @discardableResult 2 mutating func removeLast() -\u0026gt; T? { 3 // 1. Empty case 4 if head == nil { return nil } 5 6 // 2. Single element case 7 if head?.next == nil { 8 return pop() 9 } 10 11 // 3. Traverse to find node before tail 12 var prev = head 13 var current = head 14 while let next = current?.next { 15 prev = current 16 current = next 17 } 18 19 // 4. Update interactions 20 prev?.next = nil 21 tail = prev 22 return current?.value 23 } Doubly Linked List In a Singly Linked List, you can only move forward. A Doubly Linked List adds a previous pointer, allowing traversal in both directions.\nPros:\nCan traverse backwards. Deletion is O(1) if you have the node reference (no need to traverse from head). Cons:\nUses more memory (extra pointer per node). Conclusion Linked Lists are powerful tools for specific scenarios like implementing Queues, Stacks, or when you need constant-time insertions/deletions. While Swift Arrays are usually faster due to CPU caching, knowing how to build a Linked List is a badge of honor for any developer.\n","date":"2026-01-31T00:00:00Z","image":"/blogs/linked-list-swift/linked-list-structure.png","permalink":"/blogs/linked-list-swift/","title":"Mastering Linked Lists in Swift: A Comprehensive Guide"}]