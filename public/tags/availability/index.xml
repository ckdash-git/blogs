<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Availability on Chandan Kumar Dash</title>
        <link>http://localhost:53376/tags/availability/</link>
        <description>Recent content in Availability on Chandan Kumar Dash</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Sun, 08 Feb 2026 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:53376/tags/availability/index.xml" rel="self" type="application/rss+xml" /><item>
            <title>Availability &amp; Fault Tolerance: Keeping the Lights On</title>
            <link>http://localhost:53376/blogs/system-design/availability-and-fault-tolerance/</link>
            <pubDate>Sun, 08 Feb 2026 00:00:00 +0000</pubDate>
            <guid>http://localhost:53376/blogs/system-design/availability-and-fault-tolerance/</guid>
            <description>&lt;p&gt;Imagine it&amp;rsquo;s the day of your board exam results. You, along with millions of other students, log in to the result portal at 10:00 AM. The site crashes. That is an &lt;strong&gt;Availability&lt;/strong&gt; failure.&lt;/p&gt;&#xA;&lt;p&gt;In previous posts, we discussed &lt;a class=&#34;link&#34; href=&#34;http://localhost:53376/blogs/system-design/throughput-in-distributed-systems/&#34; &gt;Throughput&lt;/a&gt; and &lt;a class=&#34;link&#34; href=&#34;http://localhost:53376/blogs/system-design/latency-in-web-applications/&#34; &gt;Latency&lt;/a&gt;. Today, we tackle the most critical aspect of any production system: &lt;strong&gt;staying online&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;h2 id=&#34;core-concepts&#34;&gt;Core Concepts&#xA;&lt;/h2&gt;&lt;h3 id=&#34;1-availability&#34;&gt;1. Availability&#xA;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Availability&lt;/strong&gt; is the percentage of time a system is operational and accessible to users. It&amp;rsquo;s often measured in &amp;ldquo;nines&amp;rdquo;:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;99%&lt;/strong&gt;: Down for 3.65 days/year.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;99.99% (&amp;ldquo;Four Nines&amp;rdquo;)&lt;/strong&gt;: Down for 52 minutes/year.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;99.999% (&amp;ldquo;Five Nines&amp;rdquo;)&lt;/strong&gt;: Down for 5 minutes/year.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;2-fault-tolerance&#34;&gt;2. Fault Tolerance&#xA;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Fault Tolerance&lt;/strong&gt; is the ability of a system to continue operating properly in the event of the failure of some of its components.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;em&gt;Example&lt;/em&gt;: If one engine of an airplane fails, the plane can still fly. The plane is Fault Tolerant.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;3-single-point-of-failure-spof&#34;&gt;3. Single Point of Failure (SPOF)&#xA;&lt;/h3&gt;&lt;p&gt;A part of a system that, if it fails, stops the entire system from working.&lt;/p&gt;&#xA;&lt;h2 id=&#34;monoliths-vs-distributed-systems&#34;&gt;Monoliths vs. Distributed Systems&#xA;&lt;/h2&gt;&lt;p&gt;The architectural choice you make dictates your system&amp;rsquo;s survival strategy.&lt;/p&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Feature&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Monolithic Architecture&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Distributed System&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Structure&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;All-in-one bundle&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Modular, spread across nodes&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Failure Mode&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;SPOF&lt;/strong&gt;: If the server crashes, everything dies.&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Resilient&lt;/strong&gt;: If one node dies, others take over.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Availability&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Low (Requires downtime for updates/crashes)&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;High (Zero-downtime deployments)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Recovery&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Reboot the entire beast&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Automatic failover&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;pre class=&#34;mermaid&#34;&gt;&#xA;  graph TD&#xA;    subgraph Monolith [Monolithic System - SPOF]&#xA;        User1[User] --&amp;gt; Server[Single Server]&#xA;        Server -.-&amp;gt; Crash[X Crash]&#xA;        Crash -.-&amp;gt; Down[System Offline]&#xA;        style Server fill:#ff9999&#xA;    end&#xA;&#xA;    subgraph Distributed [Distributed System - Fault Tolerant]&#xA;        User2[User] --&amp;gt; LB[Load Balancer]&#xA;        LB --&amp;gt; NodeA[Node A]&#xA;        LB --&amp;gt; NodeB[Node B]&#xA;        NodeA -.-&amp;gt; Fail[X Fail]&#xA;        LB ==Failover==&amp;gt; NodeB&#xA;        style NodeA fill:#ff9999&#xA;        style NodeB fill:#99ff99&#xA;    end&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;h2 id=&#34;the-secret-sauce-replication&#34;&gt;The Secret Sauce: Replication&#xA;&lt;/h2&gt;&lt;p&gt;How do distributed systems achieve high availability? &lt;strong&gt;Redundancy&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;p&gt;We don&amp;rsquo;t just rely on one server. We &lt;strong&gt;Replicate&lt;/strong&gt; everything.&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;Application Replication&lt;/strong&gt;: Run the same code on 10 different servers. If 3 crash, 7 are still running.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Data Replication&lt;/strong&gt;: Store your user data on a primary database and sync it to a standby replica. If the primary burns down, the standby takes over.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Geographic Replication&lt;/strong&gt;: Don&amp;rsquo;t put all servers in one data center. If the entire underlying power grid of a region fails, your app typically keeps running from a different region.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;the-acid-trade-off&#34;&gt;The ACID Trade-off&#xA;&lt;/h3&gt;&lt;p&gt;In databases, replication introduces complexity. If you write data to Node A, it takes time to copy to Node B. This touches on the &lt;strong&gt;CAP Theorem&lt;/strong&gt; (Consistency vs. Availability), which we will cover in depth later.&lt;/p&gt;&#xA;&lt;h2 id=&#34;real-life-examples&#34;&gt;Real-Life Examples&#xA;&lt;/h2&gt;&lt;h3 id=&#34;1-amazon-shopping-prime-day&#34;&gt;1. Amazon Shopping (Prime Day)&#xA;&lt;/h3&gt;&lt;p&gt;On Prime Day, traffic spikes 100x. Amazon uses thousands of microservices distributed across the globe. If the &amp;ldquo;Reviews&amp;rdquo; service crashes, you can still buy items. The system degrades gracefully rather than failing completely.&lt;/p&gt;&#xA;&lt;h3 id=&#34;2-google-search&#34;&gt;2. Google Search&#xA;&lt;/h3&gt;&lt;p&gt;Google indexes the web across thousands of machines. If the specific server holding the index for &amp;ldquo;SpaceX&amp;rdquo; fails, a replica immediately answers your query. You, the user, never know a failure occurred.&lt;/p&gt;&#xA;&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&#xA;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Monoliths put all eggs in one basket.&lt;/strong&gt; If that basket drops, you have a mess.&#xA;&lt;strong&gt;Distributed Systems&lt;/strong&gt; accept that failure is inevitable. Hard drives die, networks cut out, and power fails. By designing with &lt;strong&gt;Fault Tolerance&lt;/strong&gt; and &lt;strong&gt;Replication&lt;/strong&gt; in mind, we build systems that can survive the chaos of the real world.&lt;/p&gt;&#xA;</description>
        </item></channel>
</rss>
