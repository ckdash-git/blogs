<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Performance on Chandan Kumar Dash</title>
        <link>http://localhost:53376/tags/performance/</link>
        <description>Recent content in Performance on Chandan Kumar Dash</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Sun, 08 Feb 2026 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:53376/tags/performance/index.xml" rel="self" type="application/rss+xml" /><item>
            <title>Caching: The Secret to Speed</title>
            <link>http://localhost:53376/blogs/system-design/caching/</link>
            <pubDate>Sun, 08 Feb 2026 00:00:00 +0000</pubDate>
            <guid>http://localhost:53376/blogs/system-design/caching/</guid>
            <description>&lt;p&gt;&lt;strong&gt;Latency&lt;/strong&gt; kills user experience. If your app takes 3 seconds to load, users leave.&#xA;The database is often the bottleneck. The solution? &lt;strong&gt;Don&amp;rsquo;t ask the database.&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;what-is-caching&#34;&gt;What is Caching?&#xA;&lt;/h2&gt;&lt;p&gt;Caching is storing frequently accessed data in a temporary storage location (usually RAM) so that future requests can be served faster.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Database (Disk)&lt;/strong&gt;: Slow. Doing a complex SQL join takes time.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Cache (RAM)&lt;/strong&gt;: Fast. Reading from memory takes microseconds.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;how-it-works-cache-hit-vs-miss&#34;&gt;How it Works: Cache Hit vs. Miss&#xA;&lt;/h2&gt;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;Request&lt;/strong&gt;: App asks for &lt;code&gt;User:123&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Cache Check&lt;/strong&gt;: Is &lt;code&gt;User:123&lt;/code&gt; in Redis?&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Yes (Cache Hit)&lt;/strong&gt;: Return data immediately. &lt;strong&gt;Fast.&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;No (Cache Miss)&lt;/strong&gt;: Fetch from DB, save to Cache, then return data. &lt;strong&gt;Slow (once).&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre class=&#34;mermaid&#34;&gt;&#xA;  %%{init: {&amp;#39;theme&amp;#39;: &amp;#39;dark&amp;#39;}}%%&#xA;graph TD&#xA;    App[Application] --&amp;gt; Check{In Cache?}&#xA;    Check -- Yes --&amp;gt; Return[Return Data]&#xA;    Check -- No --&amp;gt; DB[Fetch from DB]&#xA;    DB --&amp;gt; Save[Save to Cache]&#xA;    Save --&amp;gt; Return&#xA;    &#xA;    style Check fill:#ffcc00,stroke:#333&#xA;    style Return fill:#99ff99,stroke:#333&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;h2 id=&#34;real-world-examples&#34;&gt;Real-World Examples&#xA;&lt;/h2&gt;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Netflix&lt;/strong&gt;: Movie metadata (descriptions, actors) rarely changes. It&amp;rsquo;s cached globally.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Instagram&lt;/strong&gt;: Your profile info (bio, follower count) is cached. When you edit your profile, the cache is invalidated (cleared).&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Google&lt;/strong&gt;: Search results for common queries (&amp;ldquo;Facebook&amp;rdquo;, &amp;ldquo;Weather&amp;rdquo;) are served from cache.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;types-of-caching&#34;&gt;Types of Caching&#xA;&lt;/h2&gt;&lt;h3 id=&#34;1-in-memory-cache-local&#34;&gt;1. In-Memory Cache (Local)&#xA;&lt;/h3&gt;&lt;p&gt;Stored on the server&amp;rsquo;s own RAM.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Pros&lt;/strong&gt;: Fastest (no network call).&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Cons&lt;/strong&gt;: If server restarts, data is lost. Not shared between servers.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Tool&lt;/strong&gt;: &lt;code&gt;Guava&lt;/code&gt;, &lt;code&gt;Caffeine&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;2-distributed-cache-shared&#34;&gt;2. Distributed Cache (Shared)&#xA;&lt;/h3&gt;&lt;p&gt;A separate cluster of servers dedicated to caching.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Pros&lt;/strong&gt;: Shared across all application servers. Persistent. Scalable.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Cons&lt;/strong&gt;: Network call needed (slightly slower than local).&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Tool&lt;/strong&gt;: &lt;strong&gt;Redis&lt;/strong&gt;, &lt;strong&gt;Memcached&lt;/strong&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;3-content-delivery-network-cdn&#34;&gt;3. Content Delivery Network (CDN)&#xA;&lt;/h3&gt;&lt;p&gt;Geographically distributed servers that cache static assets (images, CSS, JS, Videos).&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Pros&lt;/strong&gt;: Serves content from a server close to the user (e.g., a London user gets data from a London server).&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Tool&lt;/strong&gt;: &lt;strong&gt;Cloudflare&lt;/strong&gt;, &lt;strong&gt;AWS CloudFront&lt;/strong&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;caching-strategy-cheat-sheet&#34;&gt;Caching Strategy Cheat Sheet&#xA;&lt;/h2&gt;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Type&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Best For&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Example Tool&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Local Cache&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Session data, heavy computations&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Caffeine&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Distributed Cache&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Database queries, API responses&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Redis&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;CDN&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Images, Videos, HTML, CSS&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Cloudflare&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h2 id=&#34;cache-invalidation-the-hard-part&#34;&gt;Cache Invalidation: The Hard Part&#xA;&lt;/h2&gt;&lt;p&gt;Data in the database changes. If the cache still has the old data, users see &lt;strong&gt;stale content&lt;/strong&gt;.&#xA;How do we fix this?&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;Time-to-Live (TTL)&lt;/strong&gt;: Automatically delete data after a set time (e.g., 5 minutes). Good for news feeds or weather.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Manual Invalidation&lt;/strong&gt;: Explicitly delete the cache when data changes (e.g., when a user updates their profile).&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&#xA;&lt;/h2&gt;&lt;p&gt;Caching is the easiest way to scale a read-heavy system.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Effect&lt;/strong&gt;: Reduces load on the database.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Trade-off&lt;/strong&gt;: Complexity in invalidation (&amp;ldquo;There are only two hard things in Computer Science: cache invalidation and naming things.&amp;rdquo;).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;</description>
        </item><item>
            <title>Throughput in Distributed Systems: The Impact of Scale</title>
            <link>http://localhost:53376/blogs/system-design/throughput-in-distributed-systems/</link>
            <pubDate>Sun, 08 Feb 2026 00:00:00 +0000</pubDate>
            <guid>http://localhost:53376/blogs/system-design/throughput-in-distributed-systems/</guid>
            <description>&lt;p&gt;In our previous discussions, we covered &lt;a class=&#34;link&#34; href=&#34;http://localhost:53376/blogs/system-design/monolithic-architecture/&#34; &gt;Monoliths&lt;/a&gt;, &lt;a class=&#34;link&#34; href=&#34;http://localhost:53376/blogs/system-design/distributed-systems/&#34; &gt;Distributed Systems&lt;/a&gt;, and &lt;a class=&#34;link&#34; href=&#34;http://localhost:53376/blogs/system-design/latency-in-web-applications/&#34; &gt;Latency&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;Today, we decode &lt;strong&gt;Throughput&lt;/strong&gt; (often measured in bits per second, bps)—the measure of how much work your system can actually handle. While Latency is about &amp;ldquo;speed,&amp;rdquo; Throughput is about &amp;ldquo;capacity.&amp;rdquo;&lt;/p&gt;&#xA;&lt;h2 id=&#34;what-is-throughput&#34;&gt;What is Throughput?&#xA;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Throughput&lt;/strong&gt; is the amount of work (requests, data, transactions) a system can process in a given unit of time.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Latency&lt;/strong&gt;: How long does it take to move &lt;em&gt;one&lt;/em&gt; car from A to B?&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Throughput&lt;/strong&gt;: How many cars &lt;em&gt;per hour&lt;/em&gt; can move from A to B?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Ideally, you want &lt;strong&gt;High Throughput&lt;/strong&gt; and &lt;strong&gt;Low Latency&lt;/strong&gt;. However, improving one can sometimes hurt the other.&lt;/p&gt;&#xA;&lt;h2 id=&#34;throughput-monolithic-vs-distributed&#34;&gt;Throughput: Monolithic vs. Distributed&#xA;&lt;/h2&gt;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Feature&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Monolithic System&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Distributed System&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Resources&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Limited to one machine (CPU/RAM cap)&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Virtually unlimited (Add more machines)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Scaling&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Vertical (Expensive, Hard limit)&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Horizontal (Add nodes endlessly)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Throughput&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Low/Capped&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;High/Uncapped&lt;/strong&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Bottleneck&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;The single server itself&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Network bandwidth or Database&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h3 id=&#34;why-distributed-systems-win-on-throughput&#34;&gt;Why Distributed Systems Win on Throughput&#xA;&lt;/h3&gt;&lt;p&gt;In a Monolith, your throughput is hard-capped by the physical limits of the server. If your server can handle 1,000 requests/sec, that&amp;rsquo;s it.&lt;/p&gt;&#xA;&lt;p&gt;In a Distributed System, you can use &lt;strong&gt;Horizontal Scaling&lt;/strong&gt;. If one server handles 1,000 req/sec, ten servers handle 10,000 req/sec.&lt;/p&gt;&#xA;&lt;pre class=&#34;mermaid&#34;&gt;&#xA;  graph TD&#xA;    LB[Load Balancer] --&amp;gt; S1[Server 1]&#xA;    LB --&amp;gt; S2[Server 2]&#xA;    LB --&amp;gt; S3[Server 3]&#xA;    S1 --&amp;gt; DB[(Database)]&#xA;    S2 --&amp;gt; DB&#xA;    S3 --&amp;gt; DB&#xA;    style LB fill:#f9f,stroke:#333&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;By adding Servers (S1, S2, S3&amp;hellip;), you increase the total throughput of the system linearly.&lt;/em&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;what-kills-throughput&#34;&gt;What Kills Throughput?&#xA;&lt;/h2&gt;&lt;p&gt;Even in distributed systems, throughput isn&amp;rsquo;t infinite. Three main factors drag it down:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;Latency&lt;/strong&gt;: If each request takes longer to process (high latency), the server is busy for longer, reducing the number of requests it can handle per second.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Protocol Overhead&lt;/strong&gt;: Every network call involves handshakes (TCP/TLS) and headers. This &amp;ldquo;administrative&amp;rdquo; data consumes bandwidth that could be used for actual payload.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Congestion&lt;/strong&gt;: When too many requests arrive at once, queues fill up. Packets get dropped, causing retries, which further clogs the system.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;strategies-to-maximize-throughput&#34;&gt;Strategies to Maximize Throughput&#xA;&lt;/h2&gt;&lt;p&gt;To get the most out of your distributed system, you need to optimize the flow of data.&lt;/p&gt;&#xA;&lt;h3 id=&#34;1-load-balancing&#34;&gt;1. Load Balancing&#xA;&lt;/h3&gt;&lt;p&gt;A &lt;strong&gt;Load Balancer&lt;/strong&gt; sits in front of your servers and distributes traffic evenly (e.g., Round Robin). This ensures no single server is overwhelmed while others sit idle.&lt;/p&gt;&#xA;&lt;h3 id=&#34;2-caching--cdns&#34;&gt;2. Caching &amp;amp; CDNs&#xA;&lt;/h3&gt;&lt;p&gt;Serving data from a cache or &lt;a class=&#34;link&#34; href=&#34;http://localhost:53376/blogs/system-design/latency-in-web-applications/&#34; &gt;CDN&lt;/a&gt; avoids hitting the backend servers entirely.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Real-Life Example&lt;/strong&gt;: &lt;strong&gt;Netflix&lt;/strong&gt;. By serving video chunks from a local ISP server (CDN), they offload terabytes of data from their main servers, massively increasing global throughput.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;3-asynchronous-processing&#34;&gt;3. Asynchronous Processing&#xA;&lt;/h3&gt;&lt;p&gt;Instead of blocking a connection while waiting for a task to finish, use message queues (like Kafka). The system accepts the request immediately (improving throughput) and processes it in the background.&lt;/p&gt;&#xA;&lt;h2 id=&#34;real-life-example-ticket-booking-vs-video-streaming&#34;&gt;Real-Life Example: Ticket Booking vs. Video Streaming&#xA;&lt;/h2&gt;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Ticket Booking (IRCTC/Ticketmaster)&lt;/strong&gt;: Low Throughput, High Consistency requirement. The bottleneck is the database lock (selling the same seat twice).&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Video Streaming (YouTube)&lt;/strong&gt;: High Throughput requirement. The system needs to push gigabytes of data per second to millions of users. It relies heavily on CDNs and distributed storage to achieve this massive throughput.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&#xA;&lt;/h2&gt;&lt;p&gt;Throughput is the capacity of your system. While Monoliths are limited by single-machine physics, Distributed Systems unlock massive throughput via Horizontal Scaling. By managing &lt;strong&gt;Congestion&lt;/strong&gt; and using &lt;strong&gt;Load Balancers&lt;/strong&gt;, you can build systems that handle millions of users effortlessly.&lt;/p&gt;&#xA;</description>
        </item><item>
            <title>Understanding Latency in Web Applications: Causes and Cures</title>
            <link>http://localhost:53376/blogs/system-design/latency-in-web-applications/</link>
            <pubDate>Sun, 08 Feb 2026 00:00:00 +0000</pubDate>
            <guid>http://localhost:53376/blogs/system-design/latency-in-web-applications/</guid>
            <description>&lt;p&gt;Speed is a feature. In system design, &lt;strong&gt;Latency&lt;/strong&gt; is the villain we are constantly fighting.&lt;/p&gt;&#xA;&lt;p&gt;In the previous post, we discussed &lt;a class=&#34;link&#34; href=&#34;http://localhost:53376/blogs/system-design/distributed-systems/&#34; &gt;Distributed Systems&lt;/a&gt;, which provide scalability at the cost of complexity. One of the biggest costs? &lt;strong&gt;Latency&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;p&gt;In this post, we&amp;rsquo;ll dissect what latency actually is, why modern distributed architectures often make it worse, and the three main weapons we have to fight it.&lt;/p&gt;&#xA;&lt;h2 id=&#34;what-is-latency&#34;&gt;What is Latency?&#xA;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Latency&lt;/strong&gt; is the time it takes for a data packet to travel from one point to another. In the context of a web application, it&amp;rsquo;s the round-trip time from the user&amp;rsquo;s action to the application&amp;rsquo;s response.&lt;/p&gt;&#xA;&lt;h3 id=&#34;the-formula-t1--t2--t3&#34;&gt;The Formula: T1 + T2 + T3&#xA;&lt;/h3&gt;&lt;p&gt;We can break down latency into three distinct phases:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;T1 (Network Delay - Request)&lt;/strong&gt;: Time for the request to travel from User → Server.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;T2 (Processing Delay - Computer)&lt;/strong&gt;: Time the server takes to think and process the request.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;T3 (Network Delay - Response)&lt;/strong&gt;: Time for the response to travel from Server → User.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;[ \text{Total Latency} = T1 + T2 + T3 ]&lt;/p&gt;&#xA;&lt;pre class=&#34;mermaid&#34;&gt;&#xA;  sequenceDiagram&#xA;    participant User&#xA;    participant Server&#xA;    Note over User, Server: T1 (Network Delay)&#xA;    User-&amp;gt;&amp;gt;Server: Request&#xA;    Note over Server: T2 (Processing)&#xA;    Note over Server, User: T3 (Network Delay)&#xA;    Server--&amp;gt;&amp;gt;User: Response&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;h2 id=&#34;architecture-impact-monolith-vs-distributed&#34;&gt;Architecture Impact: Monolith vs. Distributed&#xA;&lt;/h2&gt;&lt;p&gt;You might think that upgrading to a &amp;ldquo;modern&amp;rdquo; Distributed System (microservices) would make your app faster. &lt;strong&gt;Often, it does the opposite.&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;monolithic-architecture-low-latency&#34;&gt;Monolithic Architecture (Low Latency)&#xA;&lt;/h3&gt;&lt;p&gt;In a monolith, function calls are strictly &lt;strong&gt;in-memory&lt;/strong&gt;. When the &lt;code&gt;OrderService&lt;/code&gt; needs to check the &lt;code&gt;InventoryService&lt;/code&gt;, it&amp;rsquo;s just a function call.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Latency Profile&lt;/strong&gt;: Extremely low network overhead. Mostly T2 (Processing).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;distributed-architecture-high-latency&#34;&gt;Distributed Architecture (High Latency)&#xA;&lt;/h3&gt;&lt;p&gt;In a distributed system, that same check is now an &lt;strong&gt;HTTP request over the network&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Latency Profile&lt;/strong&gt;: Every internal communication adds a new T1 + T3 round trip.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Architecture&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Processing Delay (T2)&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Network Delay (T1 + T3)&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Overall Latency&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Monolithic&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Standard&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Minimal (External only)&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Low&lt;/strong&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Distributed&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Standard&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;High (Internal + External)&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;High&lt;/strong&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h2 id=&#34;how-to-reduce-latency&#34;&gt;How to Reduce Latency&#xA;&lt;/h2&gt;&lt;p&gt;Since we can&amp;rsquo;t change the speed of Network (T1/T3) or make CPUs infinitely fast (T2), we use architectural patterns.&lt;/p&gt;&#xA;&lt;h3 id=&#34;1-caching-reducing-t2&#34;&gt;1. Caching (Reducing T2)&#xA;&lt;/h3&gt;&lt;p&gt;If your application takes 500ms to calculate a report (T2), doing it every time is wasteful. &lt;strong&gt;Caching&lt;/strong&gt; stores the result so the next request takes 5ms.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Real-Life Example&lt;/strong&gt;: &lt;strong&gt;Twitter/X&lt;/strong&gt;. When you load your timeline, Twitter doesn&amp;rsquo;t query the database for every tweet. It pulls a pre-computed list from a Redis cache, making the load time instant.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre class=&#34;mermaid&#34;&gt;&#xA;  graph LR&#xA;    User --&amp;gt; Cache&#xA;    Cache -- Hit --&amp;gt; User&#xA;    Cache -- Miss --&amp;gt; Server&#xA;    Server --&amp;gt; Database&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;h3 id=&#34;2-content-delivery-network---cdn-reducing-t1--t3&#34;&gt;2. Content Delivery Network - CDN (Reducing T1 + T3)&#xA;&lt;/h3&gt;&lt;p&gt;If your server is in New York and your user is in Tokyo, T1 and T3 will be high because of physical distance. A &lt;strong&gt;CDN&lt;/strong&gt; stores copies of your static files (images, CSS, JS) on servers all over the world.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Real-Life Example&lt;/strong&gt;: &lt;strong&gt;Instagram&lt;/strong&gt;. The photos you see aren&amp;rsquo;t coming from a main server in the US; they are being served from a CDN edge location in your own city.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;3-hardware-scaling-reducing-t2&#34;&gt;3. Hardware Scaling (Reducing T2)&#xA;&lt;/h3&gt;&lt;p&gt;Sometimes, the simplest solution is brute force. upgrading the CPU, RAM, or using faster SSDs can significantly reduce processing time.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Benefit&lt;/strong&gt;: Easy to implement.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Downside&lt;/strong&gt;: Expensive and has limits (Vertical Scaling limit).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&#xA;&lt;/h2&gt;&lt;p&gt;Latency is the silent killer of user experience. While distributed systems offer scalability, they introduce network latency that must be managed. By understanding the &lt;strong&gt;T1+T2+T3&lt;/strong&gt; formula and aggressively using &lt;strong&gt;Caching&lt;/strong&gt; and &lt;strong&gt;CDNs&lt;/strong&gt;, you can build systems that feel instant to your users.&lt;/p&gt;&#xA;</description>
        </item></channel>
</rss>
