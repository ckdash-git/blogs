<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Optimization on Chandan Kumar Dash</title>
        <link>http://localhost:54612/tags/optimization/</link>
        <description>Recent content in Optimization on Chandan Kumar Dash</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Sun, 08 Feb 2026 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:54612/tags/optimization/index.xml" rel="self" type="application/rss+xml" /><item>
            <title>Understanding Latency in Web Applications: Causes and Cures</title>
            <link>http://localhost:54612/blogs/understanding-latency-in-web-applications-causes-and-cures/</link>
            <pubDate>Sun, 08 Feb 2026 00:00:00 +0000</pubDate>
            <guid>http://localhost:54612/blogs/understanding-latency-in-web-applications-causes-and-cures/</guid>
            <description>&lt;p&gt;Speed is a feature. In system design, &lt;strong&gt;Latency&lt;/strong&gt; is the villain we are constantly fighting.&lt;/p&gt;&#xA;&lt;p&gt;In the previous post, we discussed &lt;a class=&#34;link&#34; href=&#34;http://localhost:54612/blogs/system-design/distributed-systems/&#34; &gt;Distributed Systems&lt;/a&gt;, which provide scalability at the cost of complexity. One of the biggest costs? &lt;strong&gt;Latency&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;p&gt;In this post, we&amp;rsquo;ll dissect what latency actually is, why modern distributed architectures often make it worse, and the three main weapons we have to fight it.&lt;/p&gt;&#xA;&lt;h2 id=&#34;what-is-latency&#34;&gt;What is Latency?&#xA;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Latency&lt;/strong&gt; is the time it takes for a data packet to travel from one point to another. In the context of a web application, it&amp;rsquo;s the round-trip time from the user&amp;rsquo;s action to the application&amp;rsquo;s response.&lt;/p&gt;&#xA;&lt;h3 id=&#34;the-formula-t1--t2--t3&#34;&gt;The Formula: T1 + T2 + T3&#xA;&lt;/h3&gt;&lt;p&gt;We can break down latency into three distinct phases:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;T1 (Network Delay - Request)&lt;/strong&gt;: Time for the request to travel from User → Server.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;T2 (Processing Delay - Computer)&lt;/strong&gt;: Time the server takes to think and process the request.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;T3 (Network Delay - Response)&lt;/strong&gt;: Time for the response to travel from Server → User.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;[ \text{Total Latency} = T1 + T2 + T3 ]&lt;/p&gt;&#xA;&lt;pre class=&#34;mermaid&#34;&gt;&#xA;  sequenceDiagram&#xA;    participant User&#xA;    participant Server&#xA;    Note over User, Server: T1 (Network Delay)&#xA;    User-&amp;gt;&amp;gt;Server: Request&#xA;    Note over Server: T2 (Processing)&#xA;    Note over Server, User: T3 (Network Delay)&#xA;    Server--&amp;gt;&amp;gt;User: Response&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;h2 id=&#34;architecture-impact-monolith-vs-distributed&#34;&gt;Architecture Impact: Monolith vs. Distributed&#xA;&lt;/h2&gt;&lt;p&gt;You might think that upgrading to a &amp;ldquo;modern&amp;rdquo; Distributed System (microservices) would make your app faster. &lt;strong&gt;Often, it does the opposite.&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;monolithic-architecture-low-latency&#34;&gt;Monolithic Architecture (Low Latency)&#xA;&lt;/h3&gt;&lt;p&gt;In a monolith, function calls are strictly &lt;strong&gt;in-memory&lt;/strong&gt;. When the &lt;code&gt;OrderService&lt;/code&gt; needs to check the &lt;code&gt;InventoryService&lt;/code&gt;, it&amp;rsquo;s just a function call.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Latency Profile&lt;/strong&gt;: Extremely low network overhead. Mostly T2 (Processing).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;distributed-architecture-high-latency&#34;&gt;Distributed Architecture (High Latency)&#xA;&lt;/h3&gt;&lt;p&gt;In a distributed system, that same check is now an &lt;strong&gt;HTTP request over the network&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Latency Profile&lt;/strong&gt;: Every internal communication adds a new T1 + T3 round trip.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Architecture&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Processing Delay (T2)&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Network Delay (T1 + T3)&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Overall Latency&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Monolithic&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Standard&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Minimal (External only)&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Low&lt;/strong&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Distributed&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Standard&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;High (Internal + External)&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;High&lt;/strong&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h2 id=&#34;how-to-reduce-latency&#34;&gt;How to Reduce Latency&#xA;&lt;/h2&gt;&lt;p&gt;Since we can&amp;rsquo;t change the speed of light (T1/T3) or make CPUs infinitely fast (T2), we use architectural patterns.&lt;/p&gt;&#xA;&lt;h3 id=&#34;1-caching-reducing-t2&#34;&gt;1. Caching (Reducing T2)&#xA;&lt;/h3&gt;&lt;p&gt;If your application takes 500ms to calculate a report (T2), doing it every time is wasteful. &lt;strong&gt;Caching&lt;/strong&gt; stores the result so the next request takes 5ms.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Real-Life Example&lt;/strong&gt;: &lt;strong&gt;Twitter/X&lt;/strong&gt;. When you load your timeline, Twitter doesn&amp;rsquo;t query the database for every tweet. It pulls a pre-computed list from a Redis cache, making the load time instant.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre class=&#34;mermaid&#34;&gt;&#xA;  graph LR&#xA;    User --&amp;gt; Cache&#xA;    Cache -- Hit --&amp;gt; User&#xA;    Cache -- Miss --&amp;gt; Server&#xA;    Server --&amp;gt; Database&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;h3 id=&#34;2-content-delivery-network---cdn-reducing-t1--t3&#34;&gt;2. Content Delivery Network - CDN (Reducing T1 + T3)&#xA;&lt;/h3&gt;&lt;p&gt;If your server is in New York and your user is in Tokyo, T1 and T3 will be high because of physical distance. A &lt;strong&gt;CDN&lt;/strong&gt; stores copies of your static files (images, CSS, JS) on servers all over the world.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Real-Life Example&lt;/strong&gt;: &lt;strong&gt;Instagram&lt;/strong&gt;. The photos you see aren&amp;rsquo;t coming from a main server in the US; they are being served from a CDN edge location in your own city.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;3-hardware-scaling-reducing-t2&#34;&gt;3. Hardware Scaling (Reducing T2)&#xA;&lt;/h3&gt;&lt;p&gt;Sometimes, the simplest solution is brute force. upgrading the CPU, RAM, or using faster SSDs can significantly reduce processing time.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Benefit&lt;/strong&gt;: Easy to implement.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Downside&lt;/strong&gt;: Expensive and has limits (Vertical Scaling limit).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&#xA;&lt;/h2&gt;&lt;p&gt;Latency is the silent killer of user experience. While distributed systems offer scalability, they introduce network latency that must be managed. By understanding the &lt;strong&gt;T1+T2+T3&lt;/strong&gt; formula and aggressively using &lt;strong&gt;Caching&lt;/strong&gt; and &lt;strong&gt;CDNs&lt;/strong&gt;, you can build systems that feel instant to your users.&lt;/p&gt;&#xA;</description>
        </item></channel>
</rss>
