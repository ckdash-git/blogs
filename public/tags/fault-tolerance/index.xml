<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Fault Tolerance on Chandan Kumar Dash</title>
        <link>http://localhost:1313/tags/fault-tolerance/</link>
        <description>Recent content in Fault Tolerance on Chandan Kumar Dash</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Sun, 08 Feb 2026 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:1313/tags/fault-tolerance/index.xml" rel="self" type="application/rss+xml" /><item>
            <title>Availability &amp; Fault Tolerance: Keeping the Lights On</title>
            <link>http://localhost:1313/blogs/system-design/availability-and-fault-tolerance/</link>
            <pubDate>Sun, 08 Feb 2026 00:00:00 +0000</pubDate>
            <guid>http://localhost:1313/blogs/system-design/availability-and-fault-tolerance/</guid>
            <description>&lt;p&gt;Imagine it&amp;rsquo;s the day of your board exam results. You, along with millions of other students, log in to the result portal at 10:00 AM. The site crashes. That is an &lt;strong&gt;Availability&lt;/strong&gt; failure.&lt;/p&gt;&#xA;&lt;p&gt;In previous posts, we discussed &lt;a class=&#34;link&#34; href=&#34;http://localhost:1313/blogs/system-design/throughput-in-distributed-systems/&#34; &gt;Throughput&lt;/a&gt; and &lt;a class=&#34;link&#34; href=&#34;http://localhost:1313/blogs/system-design/latency-in-web-applications/&#34; &gt;Latency&lt;/a&gt;. Today, we tackle the most critical aspect of any production system: &lt;strong&gt;staying online&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;h2 id=&#34;core-concepts&#34;&gt;Core Concepts&#xA;&lt;/h2&gt;&lt;h3 id=&#34;1-availability&#34;&gt;1. Availability&#xA;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Availability&lt;/strong&gt; is the percentage of time a system is operational and accessible to users. It&amp;rsquo;s often measured in &amp;ldquo;nines&amp;rdquo;:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;99%&lt;/strong&gt;: Down for 3.65 days/year.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;99.99% (&amp;ldquo;Four Nines&amp;rdquo;)&lt;/strong&gt;: Down for 52 minutes/year.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;99.999% (&amp;ldquo;Five Nines&amp;rdquo;)&lt;/strong&gt;: Down for 5 minutes/year.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;2-fault-tolerance&#34;&gt;2. Fault Tolerance&#xA;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Fault Tolerance&lt;/strong&gt; is the ability of a system to continue operating properly in the event of the failure of some of its components.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;em&gt;Example&lt;/em&gt;: If one engine of an airplane fails, the plane can still fly. The plane is Fault Tolerant.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;3-single-point-of-failure-spof&#34;&gt;3. Single Point of Failure (SPOF)&#xA;&lt;/h3&gt;&lt;p&gt;A part of a system that, if it fails, stops the entire system from working.&lt;/p&gt;&#xA;&lt;h2 id=&#34;monoliths-vs-distributed-systems&#34;&gt;Monoliths vs. Distributed Systems&#xA;&lt;/h2&gt;&lt;p&gt;The architectural choice you make dictates your system&amp;rsquo;s survival strategy.&lt;/p&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Feature&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Monolithic Architecture&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Distributed System&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Structure&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;All-in-one bundle&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Modular, spread across nodes&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Failure Mode&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;SPOF&lt;/strong&gt;: If the server crashes, everything dies.&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Resilient&lt;/strong&gt;: If one node dies, others take over.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Availability&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Low (Requires downtime for updates/crashes)&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;High (Zero-downtime deployments)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Recovery&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Reboot the entire beast&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Automatic failover&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;pre class=&#34;mermaid&#34;&gt;&#xA;  graph TD&#xA;    subgraph Monolith [Monolithic System - SPOF]&#xA;        User1[User] --&amp;gt; Server[Single Server]&#xA;        Server -.-&amp;gt; Crash[X Crash]&#xA;        Crash -.-&amp;gt; Down[System Offline]&#xA;        style Server fill:#ff9999&#xA;    end&#xA;&#xA;    subgraph Distributed [Distributed System - Fault Tolerant]&#xA;        User2[User] --&amp;gt; LB[Load Balancer]&#xA;        LB --&amp;gt; NodeA[Node A]&#xA;        LB --&amp;gt; NodeB[Node B]&#xA;        NodeA -.-&amp;gt; Fail[X Fail]&#xA;        LB ==Failover==&amp;gt; NodeB&#xA;        style NodeA fill:#ff9999&#xA;        style NodeB fill:#99ff99&#xA;    end&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;h2 id=&#34;the-secret-sauce-replication&#34;&gt;The Secret Sauce: Replication&#xA;&lt;/h2&gt;&lt;p&gt;How do distributed systems achieve high availability? &lt;strong&gt;Redundancy&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;p&gt;We don&amp;rsquo;t just rely on one server. We &lt;strong&gt;Replicate&lt;/strong&gt; everything.&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;Application Replication&lt;/strong&gt;: Run the same code on 10 different servers. If 3 crash, 7 are still running.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Data Replication&lt;/strong&gt;: Store your user data on a primary database and sync it to a standby replica. If the primary burns down, the standby takes over.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Geographic Replication&lt;/strong&gt;: Don&amp;rsquo;t put all servers in one data center. If the entire underlying power grid of a region fails, your app typically keeps running from a different region.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;the-acid-trade-off&#34;&gt;The ACID Trade-off&#xA;&lt;/h3&gt;&lt;p&gt;In databases, replication introduces complexity. If you write data to Node A, it takes time to copy to Node B. This touches on the &lt;strong&gt;CAP Theorem&lt;/strong&gt; (Consistency vs. Availability), which we will cover in depth later.&lt;/p&gt;&#xA;&lt;h2 id=&#34;real-life-examples&#34;&gt;Real-Life Examples&#xA;&lt;/h2&gt;&lt;h3 id=&#34;1-amazon-shopping-prime-day&#34;&gt;1. Amazon Shopping (Prime Day)&#xA;&lt;/h3&gt;&lt;p&gt;On Prime Day, traffic spikes 100x. Amazon uses thousands of microservices distributed across the globe. If the &amp;ldquo;Reviews&amp;rdquo; service crashes, you can still buy items. The system degrades gracefully rather than failing completely.&lt;/p&gt;&#xA;&lt;h3 id=&#34;2-google-search&#34;&gt;2. Google Search&#xA;&lt;/h3&gt;&lt;p&gt;Google indexes the web across thousands of machines. If the specific server holding the index for &amp;ldquo;SpaceX&amp;rdquo; fails, a replica immediately answers your query. You, the user, never know a failure occurred.&lt;/p&gt;&#xA;&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&#xA;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Monoliths put all eggs in one basket.&lt;/strong&gt; If that basket drops, you have a mess.&#xA;&lt;strong&gt;Distributed Systems&lt;/strong&gt; accept that failure is inevitable. Hard drives die, networks cut out, and power fails. By designing with &lt;strong&gt;Fault Tolerance&lt;/strong&gt; and &lt;strong&gt;Replication&lt;/strong&gt; in mind, we build systems that can survive the chaos of the real world.&lt;/p&gt;&#xA;</description>
        </item><item>
            <title>Redundancy vs Replication: Spot the Difference</title>
            <link>http://localhost:1313/blogs/system-design/redundancy-vs-replication/</link>
            <pubDate>Sun, 08 Feb 2026 00:00:00 +0000</pubDate>
            <guid>http://localhost:1313/blogs/system-design/redundancy-vs-replication/</guid>
            <description>&lt;p&gt;These two terms are often used interchangeably, but in System Design, they mean very different things.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Redundancy&lt;/strong&gt;: &amp;ldquo;I have a spare tire.&amp;rdquo;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Replication&lt;/strong&gt;: &amp;ldquo;I have a spare tire, and it has the exact same air pressure and tread wear as the main tire.&amp;rdquo;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;1-redundancy-the-survival-strategy&#34;&gt;1. Redundancy (The Survival Strategy)&#xA;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Redundancy&lt;/strong&gt; is simply the duplication of components to increase reliability. Ideally, it&amp;rsquo;s about &lt;strong&gt;Availability&lt;/strong&gt;.&#xA;It does &lt;strong&gt;not&lt;/strong&gt; imply that the components share state or data.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Example&lt;/strong&gt;: You have two web servers running the same code. If Server A dies, Server B takes over. Since the code is static, you don&amp;rsquo;t need to &amp;ldquo;sync&amp;rdquo; anything in real-time.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;types-of-redundancy&#34;&gt;Types of Redundancy&#xA;&lt;/h3&gt;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Active Redundancy&lt;/strong&gt;: All nodes are active and sharing the load (e.g., behind a Load Balancer).&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Passive Redundancy&lt;/strong&gt;: One node is active, the others are on standby (Cold Standby).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre class=&#34;mermaid&#34;&gt;&#xA;  %%{init: {&amp;#39;theme&amp;#39;: &amp;#39;dark&amp;#39;}}%%&#xA;graph TD&#xA;    subgraph Active_Redundancy [Active Redundancy]&#xA;        LB[Load Balancer] --&amp;gt; S1[Server A]&#xA;        LB --&amp;gt; S2[Server B]&#xA;        S1 -.-&amp;gt;|Both Active| S2&#xA;    end&#xA;    &#xA;    subgraph Passive_Redundancy [Passive Redundancy]&#xA;        LB2[Load Balancer] --&amp;gt; M1[Primary]&#xA;        M1 -.-&amp;gt;|Failover| M2[Standby]&#xA;        style M2 stroke-dasharray: 5 5&#xA;    end&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;h2 id=&#34;2-replication-the-data-strategy&#34;&gt;2. Replication (The Data Strategy)&#xA;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Replication&lt;/strong&gt; is Redundancy &lt;strong&gt;+ Synchronization&lt;/strong&gt;.&#xA;It is used for &lt;strong&gt;Data&lt;/strong&gt; (Databases, Caches) where state changes over time. You can&amp;rsquo;t just have a specific &amp;ldquo;backup&amp;rdquo; DB; that backup must have the &lt;em&gt;latest&lt;/em&gt; data.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Example&lt;/strong&gt;: You have a Primary Database and a Replica. When you write &lt;code&gt;User.name = &amp;quot;Alice&amp;quot;&lt;/code&gt; to the Primary, that change must be &lt;strong&gt;Replicated&lt;/strong&gt; to the Replica.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;types-of-replication&#34;&gt;Types of Replication&#xA;&lt;/h3&gt;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Active Replication (Multi-Master)&lt;/strong&gt;: You can write to any node, and they sync with each other. Complex conflict resolution needed.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Passive Replication (Master-Slave)&lt;/strong&gt;: You write only to the Master. The Master syncs to Slaves. Slaves are Read-Only (usually).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre class=&#34;mermaid&#34;&gt;&#xA;  %%{init: {&amp;#39;theme&amp;#39;: &amp;#39;dark&amp;#39;}}%%&#xA;graph TD&#xA;    Client --&amp;gt;|Write| Master[(Master DB)]&#xA;    Master --&amp;gt;|Sync Data| Slave1[(Replica 1)]&#xA;    Master --&amp;gt;|Sync Data| Slave2[(Replica 2)]&#xA;    Client --&amp;gt;|Read| Slave1&#xA;    Client --&amp;gt;|Read| Slave2&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;h2 id=&#34;comparison-cheat-sheet&#34;&gt;Comparison Cheat Sheet&#xA;&lt;/h2&gt;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Feature&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Redundancy&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Replication&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Goal&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Availability&lt;/strong&gt; (Survive failure)&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Consistency&lt;/strong&gt; (Keep data in sync)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Sync Needed?&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;No (Usually stateless)&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Yes&lt;/strong&gt; (Critical)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Use Case&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Web Servers, Power Supplies, Network Cables&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Databases, Caches, File Storage&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Complexity&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Low&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;High (Handling lag, conflicts)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&#xA;&lt;/h2&gt;&lt;ul&gt;&#xA;&lt;li&gt;Use &lt;strong&gt;Redundancy&lt;/strong&gt; to keep your application running (Stateless).&lt;/li&gt;&#xA;&lt;li&gt;Use &lt;strong&gt;Replication&lt;/strong&gt; to keep your data safe and consistent (Stateful).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;</description>
        </item></channel>
</rss>
